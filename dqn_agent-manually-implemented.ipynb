{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30fc2e2-8c7d-438b-b1fb-b30de9816711",
   "metadata": {},
   "source": [
    "Enhancements:\n",
    "- Include price that assets were bought into the state\n",
    "- Explore neutral penalty\n",
    "- Encoding layer to control/learn state representation\n",
    "- Improve stability of learning algorithms through the use of PPO, SAC\n",
    "- Create short-term endpoints for trading windows during training and shuffle these windows\n",
    "- Feature selection and hyperparameter tuning through validation search [Discount rate, ]\n",
    "- More sophistication for state-propagation, e.g. Recurrent NN, Transformer\n",
    "\n",
    "TODO:\n",
    "- Walk-forward validation training\n",
    "- Test evaluation of all models\n",
    "- Use trained value network and pretrained policy network?\n",
    "\n",
    "\n",
    "Consider:\n",
    "- Recreating the table on Slide 41 of 56 from this set of lecture slides would be good for our purposes https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf\n",
    "\n",
    "\n",
    "Final report\n",
    "- Describe RL\n",
    "- Describe LOB\n",
    "- What metric are we trying to optimise\n",
    "- Inputs\n",
    "- Literature review\n",
    "- Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e4a4d-1b44-4f56-9fac-0cd2fd660a44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Overview\n",
    "\n",
    "Note that for this Reinforcement Learning problem, the actions available to the agent are:\n",
    "- 0: Short   (Have -1 asset)\n",
    "- 1: Neutral (Have 0 asset)\n",
    "- 2: Long    (Have 1 asset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f4020-f9bb-40b5-ba47-c0c3eb1acc21",
   "metadata": {},
   "source": [
    "## 2. Install the necessary packages\n",
    "- Models\n",
    "- Commons\n",
    "- Environment manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4925c3c2-4e00-4389-9bd8-721f04f31b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/e0310734/code/requirements_installer\n",
      "Installing collected packages: rl-finance\n",
      "  Attempting uninstall: rl-finance\n",
      "    Found existing installation: rl-finance 0.1\n",
      "    Uninstalling rl-finance-0.1:\n",
      "      Successfully uninstalled rl-finance-0.1\n",
      "  Running setup.py develop for rl-finance\n",
      "Successfully installed rl-finance-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e requirements_installer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51a774-c6a4-4933-bfd4-119b5d640c4e",
   "metadata": {},
   "source": [
    "## 3. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "03432b8f-6499-49d9-aece-158eaf854b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from rl_finance.environments import BaseEnvironment\n",
    "from rl_finance.commons import Experience, ReplayMemory\n",
    "from rl_finance.models import ActionValueNetwork, PolicyNetwork\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "mlflow.set_tracking_uri(\"file:///home/e0310734/logs/mlruns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd33c5-0ed9-4828-ac8d-3f812d7d5a2a",
   "metadata": {},
   "source": [
    "## 4. List the dataset locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0e6134-47de-4b29-b506-e668e81c54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTH = 'jan'\n",
    "datasets = []\n",
    "dataset_scopes = ['minimal', 'minimal_window', 'full', 'full_window']\n",
    "dataset_components = [\n",
    "    'X_train.pt', 'X_validation.pt', 'X_test.pt',\n",
    "    'y_train.pt', 'y_validation.pt', 'y_test.pt',\n",
    "    'bid_train.pt', 'bid_val.pt', 'bid_test.pt',\n",
    "    'ask_train.pt', 'ask_val.pt', 'ask_test.pt',\n",
    "]\n",
    "\n",
    "for dataset_scope in dataset_scopes:\n",
    "    dataset = []\n",
    "    for dataset_component in dataset_components:\n",
    "        dataset.append(\"../data/processed_datasets/\" + MONTH + \"/\" + dataset_scope + \"/\" + dataset_component)\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53410a62-3c77-4561-bf71-3f54236347fa",
   "metadata": {},
   "source": [
    "## 5. State constant hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a82735c-3427-4ec5-8ab1-176eebb4247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SPACE = [0, 1, 2]\n",
    "OUTPUT_DIMS = 3\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "N_EPOCHS = 150\n",
    "EPOCH_TRAIN_START = 5  # Number of epochs to populate the ReplayMemory before we start sampling from it\n",
    "LEARNING_RATE = 1e-6\n",
    "REPLAY_MEMORY_SIZE = int(2**20)\n",
    "DISCOUNT_RATE = 0.999\n",
    "FROZEN_UPDATE_INTERVAL = 12\n",
    "\n",
    "TRADING_FEE = 3e-5\n",
    "NEUTRAL_PENALTY = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576599e0-b83e-4028-b997-e2382b4f9892",
   "metadata": {},
   "source": [
    "## 6. Select exploration-exploitation trade-off parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba03b63-add1-428c-a92d-ee18e6759db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_exploration_rate = 0.001\n",
    "max_exploration_rate = 1.\n",
    "exploration_decay_rate = 0.03\n",
    "ers = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*(epoch - EPOCH_TRAIN_START))\n",
    "    ers.append(exploration_rate)\n",
    "pd.Series(ers).plot()\n",
    "plt.title(\"Probability that an agent will explore\")\n",
    "plt.ylabel(\"Exploration rate\")\n",
    "plt.ylim((0,1))\n",
    "plt.xlabel(\"Epoch number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a8061b-9c65-4d76-bc0f-02610c04f8ab",
   "metadata": {},
   "source": [
    "## 7. Reinforcement Learning: Deep Q Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b333da-429c-40bf-b7e7-1c1e6ae47398",
   "metadata": {},
   "source": [
    "Start with a 1-pass algorithm then package it into a function and iterate over each of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a26bae74-dac3-4390-9423-0dd2238f96a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/e0310734/logs/mlruns/2', experiment_id='2', lifecycle_stage='active', name='DQN', tags={}>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"DQN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0562d949-935e-4206-aebf-a75776404260",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, bid_train, bid_val, bid_test, ask_train, ask_val, ask_test = [torch.load(path) for path in datasets[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97438aca-4776-4577-bdd3-29cc9a1b7714",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = X_train.shape[-1]\n",
    "OUTPUT_DIM = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "718e183a-c691-4b91-a19c-291d218f9d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BaseEnvironment(X_train, bid_train, ask_train, use_midprice=False)\n",
    "replay_memory = ReplayMemory(REPLAY_MEMORY_SIZE)\n",
    "\n",
    "action_value_network = ActionValueNetwork(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)\n",
    "action_value_network.to(device)\n",
    "\n",
    "target_action_value_network = ActionValueNetwork(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)\n",
    "target_action_value_network.load_state_dict(action_value_network.state_dict())\n",
    "target_action_value_network.to(device)\n",
    "\n",
    "action_value_network_optimizer = optim.Adam(params=action_value_network.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8d2093c4-c431-4275-9c63-1b346e57b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_TRAIN_START = 1\n",
    "BATCH_SIZE = 5\n",
    "N_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "04d3d9b7-cc08-4920-9df0-9e64a42364a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13630/1069374711.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m# Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0maction_value_network_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0maction_value_network_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl_base/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    215\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "action_value_loss_history = []\n",
    "reward_history = []\n",
    "\n",
    "best_episode_reward = float(\"-inf\")\n",
    "best_episode_validation_reward = float(\"-inf\")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    action_value_network.train()\n",
    "    \n",
    "    episode_action_history = []\n",
    "    episode_reward = 0\n",
    "    \n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        # Exploration vs Exploitation\n",
    "        exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*(epoch - EPOCH_TRAIN_START))\n",
    "        if random.uniform(0, 1) > exploration_rate:\n",
    "            action_value_network(state).argmax()\n",
    "        else:\n",
    "            action = torch.tensor(random.sample(ACTION_SPACE, 1)).to(device) # Explore\n",
    "        \n",
    "        next_state, reward, done = env.step(action.item())\n",
    "        \n",
    "        # Saves\n",
    "        replay_memory.push(Experience(state, action, reward, next_state, done))\n",
    "        episode_action_history.append(action.item())\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if (epoch >= EPOCH_TRAIN_START) and (replay_memory.can_provide_sample(BATCH_SIZE)):\n",
    "            states, actions, rewards, next_states, dones = replay_memory.sample(BATCH_SIZE)\n",
    "            \n",
    "            states = torch.stack(states, dim=0).to(device)\n",
    "            actions = torch.cat(actions, dim=0).to(device)\n",
    "            rewards = torch.tensor(rewards).float().to(device)\n",
    "            next_states = torch.stack(next_states, dim=0).to(device)\n",
    "            dones = torch.tensor(dones).float().to(device)\n",
    "            \n",
    "            # Error calculation\n",
    "            # action_targets = value_network(next_states).max(dim=-1)[1].unsqueeze(-1)\n",
    "            # target_values = rewards + (1 - dones) * DISCOUNT_RATE * fixed_network(next_states).gather(-1, action_targets).squeeze(-1) # Double DQN\n",
    "            target_values = rewards + (1 - dones) * DISCOUNT_RATE * target_action_value_network(next_states).max(dim=-1)[0] # DQN\n",
    "            target_values = target_values.detach()\n",
    "\n",
    "            current_values = action_value_network(states).gather(-1, actions.unsqueeze(-1)).squeeze(1)\n",
    "\n",
    "            # Loss\n",
    "            loss = loss_fn(current_values, target_values)\n",
    "            action_value_network_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            action_value_network_optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2891bd10-9b7d-4055-b9a1-088ae5d0d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "86733e49-2399-433c-baed-cb9af2340b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b4ffc219-a10a-4c8c-b319-7cba53e9c1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4c285d09-b767-4313-b6b9-db11d7917b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51972f-73de-45d6-8e80-8fd4fa77b021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "83f9fa8d-0255-438c-89b3-a052591832c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mExperience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      Experience(state, action, reward, next_state, done)\n",
       "\u001b[0;31mFile:\u001b[0m           ~/code/requirements_installer/rl_finance/commons/experience.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Experience?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94038f61-971c-46c1-a811-ad0ef11d488c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb01cb-0efc-4eb9-a04b-0f93d2ff8315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982daa5c-e921-493c-a2e4-2d518908132a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ce191-b56c-451d-9428-70fbb0e4cb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727f6fc-73e2-4737-bc94-96da0ba98b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e7943e7a-87b9-4575-b6a9-860d7572d6bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'value_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13630/2548687851.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mvalue_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mepisode_action_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'value_network' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    value_network.train()\n",
    "    episode_action_history = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    state = X_train[0].clone()\n",
    "    bid, ask = bid_train[0], ask_train[0]\n",
    "\n",
    "    for i in range(1, X_train.shape[0]):\n",
    "        # Explore vs exploit\n",
    "        exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*(epoch - EPOCH_TRAIN_START))\n",
    "        if random.uniform(0, 1) > exploration_rate:\n",
    "            value_network(state).argmax()\n",
    "        else:\n",
    "            action = torch.tensor(random.sample(ACTION_SPACE, 1)).to(device) # Explore\n",
    "\n",
    "        # Environment step\n",
    "        next_state = X_train[i].clone()\n",
    "        next_state[..., -1] = action\n",
    "        next_bid = bid_train[i]\n",
    "        next_ask = ask_train[i]\n",
    "\n",
    "        reward = get_reward(state, bid, ask, next_state, next_bid, next_ask, use_midprice=USE_MIDPRICE, trading_fee=TRADING_FEE, neutral_penalty=NEUTRAL_PENALTY)\n",
    "        done = i == (X_train.shape[0] - 1)\n",
    "\n",
    "        # Saves\n",
    "        replay_memory.push(Experience(state, action, reward, next_state, done))\n",
    "        episode_action_history.append(action.item())\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        bid = next_bid\n",
    "        ask = next_ask\n",
    "\n",
    "        if replay_memory.can_provide_sample(BATCH_SIZE) and epoch > EPOCH_TRAIN_START:\n",
    "            states, actions, rewards, next_states, dones = replay_memory.sample(BATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = states.to(device), actions.to(device), rewards.to(device), next_states.to(device), dones.to(device)\n",
    "            actions = actions.long()\n",
    "            dones = dones.float()\n",
    "\n",
    "            # Error calculation\n",
    "            action_targets = value_network(next_states).max(dim=-1)[1].unsqueeze(-1)\n",
    "            target_values = rewards + (1 - dones) * DISCOUNT_RATE * fixed_network(next_states).gather(-1, action_targets).squeeze(-1) # Double DQN\n",
    "            # target_values = rewards + (1 - dones) * DISCOUNT_RATE * q_fn_frozen(next_states).max(dim=-1)[0] # DQN\n",
    "\n",
    "            current_values = value_network(states).gather(-1, actions.unsqueeze(-1)).squeeze(1)\n",
    "\n",
    "            # Loss\n",
    "            loss = mse_fn(current_values, target_values.detach())\n",
    "            value_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            value_optimizer.step()\n",
    "\n",
    "    reward_history.append(episode_reward) ######## LOG THE REWARD\n",
    "    mlflow.log_metric(\"episode_reward\", episode_reward, step=epoch)\n",
    "\n",
    "    if replay_memory.can_provide_sample(BATCH_SIZE) and i % FROZEN_UPDATE_INTERVAL == 0 and epoch > EPOCH_TRAIN_START:\n",
    "        fixed_network.load_state_dict(value_network.state_dict())\n",
    "        fixed_network.eval()\n",
    "\n",
    "    if replay_memory.can_provide_sample(BATCH_SIZE) and epoch > EPOCH_TRAIN_START:\n",
    "            print(f\"Epoch: {epoch:>5,d}, Reward: {episode_reward:>10,.6f}, Value loss: {loss.item():>10,.6f}\", end=\"\\r\")\n",
    "            mlflow.log_metric(\"train_value_network_loss\", loss.item(), step=epoch)\n",
    "\n",
    "            # Save episode action information - Need to see if the agent is changing its portfolio regularly\n",
    "            for k, v in dict(pd.Series(episode_action_history).value_counts()).items():\n",
    "                mlflow.log_metric(f\"action_{k}\", v, step=epoch)\n",
    "\n",
    "    if (episode_reward > best_episode_reward):\n",
    "        best_episode_reward = episode_reward\n",
    "        torch.save(value_network.state_dict(), \"best_train_value_state_dict.pt\")\n",
    "        mlflow.log_artifact(\"best_train_value_state_dict.pt\")\n",
    "################\n",
    "    with torch.no_grad():\n",
    "        # Validation checks\n",
    "        value_network.eval()\n",
    "        val_state = X_val[0]\n",
    "        val_bid = bid_val[0]\n",
    "        val_ask = ask_val[0]\n",
    "        validation_reward = 0\n",
    "        validation_actions = []\n",
    "\n",
    "        for k in range(1, X_val.shape[0]):\n",
    "            val_action = value_network(val_state).argmax()\n",
    "\n",
    "            val_next_state = X_val[k]\n",
    "            val_next_state[..., -1] = val_action\n",
    "\n",
    "            val_next_bid = bid_val[k]\n",
    "            val_next_ask = ask_val[k]\n",
    "\n",
    "            reward = get_reward(val_state, val_bid, val_ask, val_next_state, val_next_bid, val_next_ask, neutral_penalty=0)\n",
    "            validation_reward += reward\n",
    "            validation_actions.append(val_action.item())\n",
    "            val_state = val_next_state\n",
    "            val_bid = val_next_bid\n",
    "            val_ask = val_next_ask\n",
    "\n",
    "        mlflow.log_metric(\"validation_episode_reward\", validation_reward, step=epoch)\n",
    "        for key, value in dict(pd.Series(validation_actions).value_counts()).items():\n",
    "            mlflow.log_metric(f\"validation_action_{key}\", value, step=epoch)\n",
    "\n",
    "        if validation_reward > best_episode_validation_reward:\n",
    "            best_episode_validation_reward = validation_reward\n",
    "            torch.save(value_network.state_dict(), \"best_val_value_state_dict.pt\")\n",
    "            mlflow.log_artifact(\"best_val_value_state_dict.pt\")\n",
    "        value_network.train()\n",
    "\n",
    "torch.save(value_network.state_dict(), \"last_train_value_state_dict.pt\")\n",
    "mlflow.log_artifact(\"last_train_value_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdf3e2-2776-4f2d-a9d5-344397407d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad4ea0-6ba3-4b46-a3d1-fbcafa3cbd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56281de0-8424-4686-b1ef-bfb9aced9e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e1ec8-6d0e-4a86-8287-7e8b4533f121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    14, Reward: -29.872640, Value loss:   0.241062\r"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    for USE_MIDPRICE in [False, True]:\n",
    "        # Fetch data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, bid_train, bid_val, bid_test, ask_train, ask_val, ask_test = [torch.load(path) for path in dataset]\n",
    "        y_train, y_val, y_test = y_train.long(), y_val.long(), y_test.long()\n",
    "        INPUT_DIMS = X_train.shape[-1]\n",
    "        \n",
    "        value_network = ValueNetwork(input_dim = INPUT_DIMS, output_dim = OUTPUT_DIMS).to(device)\n",
    "        fixed_network = ValueNetwork(input_dim = INPUT_DIMS, output_dim = OUTPUT_DIMS).to(device)\n",
    "        fixed_network.load_state_dict(value_network.state_dict())\n",
    "        value_network.train()\n",
    "        fixed_network.eval()\n",
    "        \n",
    "        value_optimizer = optim.Adam(params=value_network.parameters(), lr=LEARNING_RATE)\n",
    "        mse_fn = nn.MSELoss()\n",
    "        \n",
    "        # Begin logging\n",
    "        with mlflow.start_run() as run:\n",
    "            mlflow.log_params({\n",
    "                'batch_size': BATCH_SIZE,\n",
    "                'n_epochs': N_EPOCHS,\n",
    "                'train_start_epoch': EPOCH_TRAIN_START,\n",
    "                'learning_rate': LEARNING_RATE,\n",
    "                'replay_memory_size': REPLAY_MEMORY_SIZE,\n",
    "                'discount_rate': DISCOUNT_RATE,\n",
    "                'frozen_update_interval': FROZEN_UPDATE_INTERVAL,\n",
    "                'use_midprice': USE_MIDPRICE,\n",
    "                'trading_fee': TRADING_FEE,\n",
    "                'neutral_penalty': NEUTRAL_PENALTY,\n",
    "                'market_open': MARKET_OPEN,\n",
    "                'market_close': MARKET_CLOSE,\n",
    "                'granularity': GRANULARITY,\n",
    "                'month': MONTH,\n",
    "                'datascope': dataset[0].split(\"/\")[2],\n",
    "                'device': device,\n",
    "                'input_dims': INPUT_DIMS,\n",
    "                'critic_loss_fn': 'MSE',\n",
    "                'network_optimizer_fns': 'Adam',\n",
    "                'pretrain_loss_fn': 'CrossEntropyLoss',\n",
    "                'policy_network': str(value_network.linear_stack).replace(\"in_features=\", \"\").replace(\"out_features=\", \"\").replace(\" \", \"\").replace(\"\\n\", \"\").replace(\",bias=True\", \"\"),\n",
    "                'min_exploration_rate': min_exploration_rate,\n",
    "                'max_exploration_rate': max_exploration_rate,\n",
    "                'exploration_decay_rate': exploration_decay_rate,\n",
    "            })\n",
    "\n",
    "            critic_loss_history = []\n",
    "            policy_loss_history = []\n",
    "            reward_history = []\n",
    "            replay_memory = ReplayMemory(REPLAY_MEMORY_SIZE) # 2**20 values\n",
    "            \n",
    "            best_episode_reward = float(\"-inf\")\n",
    "            best_episode_validation_reward = float(\"-inf\")\n",
    "\n",
    "            for epoch in range(N_EPOCHS):\n",
    "                value_network.train()\n",
    "                episode_action_history = []\n",
    "                done = False\n",
    "                episode_reward = 0\n",
    "                state = X_train[0].clone()\n",
    "                bid, ask = bid_train[0], ask_train[0]\n",
    "\n",
    "                for i in range(1, X_train.shape[0]):\n",
    "                    # Explore vs exploit\n",
    "                    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*(epoch - EPOCH_TRAIN_START))\n",
    "                    if random.uniform(0, 1) > exploration_rate:\n",
    "                        value_network(state).argmax()\n",
    "                    else:\n",
    "                        action = torch.tensor(random.sample(ACTION_SPACE, 1)).to(device) # Explore\n",
    "\n",
    "                    # Environment step\n",
    "                    next_state = X_train[i].clone()\n",
    "                    next_state[..., -1] = action\n",
    "                    next_bid = bid_train[i]\n",
    "                    next_ask = ask_train[i]\n",
    "\n",
    "                    reward = get_reward(state, bid, ask, next_state, next_bid, next_ask, use_midprice=USE_MIDPRICE, trading_fee=TRADING_FEE, neutral_penalty=NEUTRAL_PENALTY)\n",
    "                    done = i == (X_train.shape[0] - 1)\n",
    "\n",
    "                    # Saves\n",
    "                    replay_memory.push(Experience(state, action, reward, next_state, done))\n",
    "                    episode_action_history.append(action.item())\n",
    "                    episode_reward += reward\n",
    "                    state = next_state\n",
    "                    bid = next_bid\n",
    "                    ask = next_ask\n",
    "\n",
    "                    if replay_memory.can_provide_sample(BATCH_SIZE) and epoch > EPOCH_TRAIN_START:\n",
    "                        states, actions, rewards, next_states, dones = replay_memory.sample(BATCH_SIZE)\n",
    "                        states, actions, rewards, next_states, dones = states.to(device), actions.to(device), rewards.to(device), next_states.to(device), dones.to(device)\n",
    "                        actions = actions.long()\n",
    "                        dones = dones.float()\n",
    "\n",
    "                        # Error calculation\n",
    "                        action_targets = value_network(next_states).max(dim=-1)[1].unsqueeze(-1)\n",
    "                        target_values = rewards + (1 - dones) * DISCOUNT_RATE * fixed_network(next_states).gather(-1, action_targets).squeeze(-1) # Double DQN\n",
    "                        # target_values = rewards + (1 - dones) * DISCOUNT_RATE * q_fn_frozen(next_states).max(dim=-1)[0] # DQN\n",
    "\n",
    "                        current_values = value_network(states).gather(-1, actions.unsqueeze(-1)).squeeze(1)\n",
    "\n",
    "                        # Loss\n",
    "                        loss = mse_fn(current_values, target_values.detach())\n",
    "                        value_optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        value_optimizer.step()\n",
    "\n",
    "                reward_history.append(episode_reward) ######## LOG THE REWARD\n",
    "                mlflow.log_metric(\"episode_reward\", episode_reward, step=epoch)\n",
    "\n",
    "                if replay_memory.can_provide_sample(BATCH_SIZE) and i % FROZEN_UPDATE_INTERVAL == 0 and epoch > EPOCH_TRAIN_START:\n",
    "                    fixed_network.load_state_dict(value_network.state_dict())\n",
    "                    fixed_network.eval()\n",
    "\n",
    "                if replay_memory.can_provide_sample(BATCH_SIZE) and epoch > EPOCH_TRAIN_START:\n",
    "                        print(f\"Epoch: {epoch:>5,d}, Reward: {episode_reward:>10,.6f}, Value loss: {loss.item():>10,.6f}\", end=\"\\r\")\n",
    "                        mlflow.log_metric(\"train_value_network_loss\", loss.item(), step=epoch)\n",
    "                        \n",
    "                        # Save episode action information - Need to see if the agent is changing its portfolio regularly\n",
    "                        for k, v in dict(pd.Series(episode_action_history).value_counts()).items():\n",
    "                            mlflow.log_metric(f\"action_{k}\", v, step=epoch)\n",
    "                            \n",
    "                if (episode_reward > best_episode_reward):\n",
    "                    best_episode_reward = episode_reward\n",
    "                    torch.save(value_network.state_dict(), \"best_train_value_state_dict.pt\")\n",
    "                    mlflow.log_artifact(\"best_train_value_state_dict.pt\")\n",
    "################\n",
    "                with torch.no_grad():\n",
    "                    # Validation checks\n",
    "                    value_network.eval()\n",
    "                    val_state = X_val[0]\n",
    "                    val_bid = bid_val[0]\n",
    "                    val_ask = ask_val[0]\n",
    "                    validation_reward = 0\n",
    "                    validation_actions = []\n",
    "\n",
    "                    for k in range(1, X_val.shape[0]):\n",
    "                        val_action = value_network(val_state).argmax()\n",
    "\n",
    "                        val_next_state = X_val[k]\n",
    "                        val_next_state[..., -1] = val_action\n",
    "\n",
    "                        val_next_bid = bid_val[k]\n",
    "                        val_next_ask = ask_val[k]\n",
    "\n",
    "                        reward = get_reward(val_state, val_bid, val_ask, val_next_state, val_next_bid, val_next_ask, neutral_penalty=0)\n",
    "                        validation_reward += reward\n",
    "                        validation_actions.append(val_action.item())\n",
    "                        val_state = val_next_state\n",
    "                        val_bid = val_next_bid\n",
    "                        val_ask = val_next_ask\n",
    "\n",
    "                    mlflow.log_metric(\"validation_episode_reward\", validation_reward, step=epoch)\n",
    "                    for key, value in dict(pd.Series(validation_actions).value_counts()).items():\n",
    "                        mlflow.log_metric(f\"validation_action_{key}\", value, step=epoch)\n",
    "\n",
    "                    if validation_reward > best_episode_validation_reward:\n",
    "                        best_episode_validation_reward = validation_reward\n",
    "                        torch.save(value_network.state_dict(), \"best_val_value_state_dict.pt\")\n",
    "                        mlflow.log_artifact(\"best_val_value_state_dict.pt\")\n",
    "                    value_network.train()\n",
    "\n",
    "            torch.save(value_network.state_dict(), \"last_train_value_state_dict.pt\")\n",
    "            mlflow.log_artifact(\"last_train_value_state_dict.pt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e44b0-d3ed-4521-9fa3-61cb55d775a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DQN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
