{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b30fc2e2-8c7d-438b-b1fb-b30de9816711",
   "metadata": {},
   "source": [
    "Enhancements:\n",
    "- Include price that assets were bought into the state\n",
    "- Explore neutral penalty\n",
    "- Encoding layer to control/learn state representation\n",
    "- Improve stability of learning algorithms through the use of PPO, SAC\n",
    "- Create short-term endpoints for trading windows during training and shuffle these windows\n",
    "- Feature selection and hyperparameter tuning through validation search [Discount rate, ]\n",
    "- More sophistication for state-propagation, e.g. Recurrent NN, Transformer\n",
    "\n",
    "TODO:\n",
    "- Walk-forward validation training\n",
    "- Test evaluation of all models\n",
    "- Use trained value network and pretrained policy network?\n",
    "\n",
    "\n",
    "Consider:\n",
    "- Recreating the table on Slide 41 of 56 from this set of lecture slides would be good for our purposes https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf\n",
    "\n",
    "\n",
    "Final report\n",
    "- Describe RL\n",
    "- Describe LOB\n",
    "- What metric are we trying to optimise\n",
    "- Inputs\n",
    "- Literature review\n",
    "- Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e4a4d-1b44-4f56-9fac-0cd2fd660a44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Overview\n",
    "\n",
    "Note that for this Reinforcement Learning problem, the actions available to the agent are:\n",
    "- 0: Short   (Have -1 asset)\n",
    "- 1: Neutral (Have 0 asset)\n",
    "- 2: Long    (Have 1 asset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f4020-f9bb-40b5-ba47-c0c3eb1acc21",
   "metadata": {},
   "source": [
    "## 2. Install the necessary packages\n",
    "- Models\n",
    "- Commons\n",
    "- Environment manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4925c3c2-4e00-4389-9bd8-721f04f31b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/e0310734/code/requirements_installer\n",
      "Installing collected packages: rl-finance\n",
      "  Attempting uninstall: rl-finance\n",
      "    Found existing installation: rl-finance 0.1\n",
      "    Uninstalling rl-finance-0.1:\n",
      "      Successfully uninstalled rl-finance-0.1\n",
      "  Running setup.py develop for rl-finance\n",
      "Successfully installed rl-finance-0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -e requirements_installer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51a774-c6a4-4933-bfd4-119b5d640c4e",
   "metadata": {},
   "source": [
    "## 3. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03432b8f-6499-49d9-aece-158eaf854b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import random\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from rl_finance.environments import BaseEnvironment\n",
    "from rl_finance.commons import Experience, ReplayMemory\n",
    "from rl_finance.models import ActionValueNetwork, PolicyNetwork\n",
    "from rl_finance.agents import DQN_Agent\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "mlflow.set_tracking_uri(\"file:///home/e0310734/logs/mlruns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd33c5-0ed9-4828-ac8d-3f812d7d5a2a",
   "metadata": {},
   "source": [
    "## 4. List the dataset locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0e6134-47de-4b29-b506-e668e81c54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTH = 'jan'\n",
    "datasets = []\n",
    "dataset_scopes = ['minimal', 'minimal_window', 'full', 'full_window']\n",
    "dataset_components = [\n",
    "    'X_train.pt', 'X_validation.pt', 'X_test.pt',\n",
    "    'y_train.pt', 'y_validation.pt', 'y_test.pt',\n",
    "    'bid_train.pt', 'bid_val.pt', 'bid_test.pt',\n",
    "    'ask_train.pt', 'ask_val.pt', 'ask_test.pt',\n",
    "]\n",
    "\n",
    "for dataset_scope in dataset_scopes:\n",
    "    dataset = []\n",
    "    for dataset_component in dataset_components:\n",
    "        dataset.append(\"../data/processed_datasets/\" + MONTH + \"/\" + dataset_scope + \"/\" + dataset_component)\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53410a62-3c77-4561-bf71-3f54236347fa",
   "metadata": {},
   "source": [
    "## 5. State constant hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a82735c-3427-4ec5-8ab1-176eebb4247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SPACE = [0, 1, 2]\n",
    "OUTPUT_DIMS = 3\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "N_EPOCHS = 150\n",
    "EPOCH_TRAIN_START = 5  # Number of epochs to populate the ReplayMemory before we start sampling from it\n",
    "LEARNING_RATE = 1e-6\n",
    "REPLAY_MEMORY_SIZE = int(2**20)\n",
    "DISCOUNT_RATE = 0.999\n",
    "FROZEN_UPDATE_INTERVAL = 12\n",
    "\n",
    "TRADING_FEE = 3e-5\n",
    "NEUTRAL_PENALTY = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576599e0-b83e-4028-b997-e2382b4f9892",
   "metadata": {},
   "source": [
    "## 6. Select exploration-exploitation trade-off parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ba03b63-add1-428c-a92d-ee18e6759db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_expl = 0.001\n",
    "max_expl = 1.\n",
    "expl_decay = 0.03\n",
    "ers = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    exploration_rate = min_expl + (max_expl - min_expl) * np.exp(-expl_decay*(epoch - EPOCH_TRAIN_START))\n",
    "    ers.append(exploration_rate)\n",
    "pd.Series(ers).plot()\n",
    "plt.title(\"Probability that an agent will explore\")\n",
    "plt.ylabel(\"Exploration rate\")\n",
    "plt.ylim((0,1))\n",
    "plt.xlabel(\"Epoch number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a8061b-9c65-4d76-bc0f-02610c04f8ab",
   "metadata": {},
   "source": [
    "## 7. Reinforcement Learning: Deep Q Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b333da-429c-40bf-b7e7-1c1e6ae47398",
   "metadata": {},
   "source": [
    "Start with a 1-pass algorithm then package it into a function and iterate over each of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a26bae74-dac3-4390-9423-0dd2238f96a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/e0310734/logs/mlruns/2', experiment_id='2', lifecycle_stage='active', name='DQN', tags={}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"DQN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0562d949-935e-4206-aebf-a75776404260",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, bid_train, bid_val, bid_test, ask_train, ask_val, ask_test = [torch.load(path) for path in datasets[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97438aca-4776-4577-bdd3-29cc9a1b7714",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = X_train.shape[-1]\n",
    "OUTPUT_DIM = len(ACTION_SPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "718e183a-c691-4b91-a19c-291d218f9d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = BaseEnvironment(X_train, bid_train, ask_train, use_midprice=False)\n",
    "val_env = BaseEnvironment(X_val, bid_val, ask_val, use_midprice=False)\n",
    "test_env = BaseEnvironment(X_test, bid_test, ask_test, use_midprice=False)\n",
    "replay_memory = ReplayMemory(REPLAY_MEMORY_SIZE)\n",
    "\n",
    "action_value_network = ActionValueNetwork(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)\n",
    "action_value_network.to(device)\n",
    "\n",
    "target_action_value_network = ActionValueNetwork(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)\n",
    "target_action_value_network.load_state_dict(action_value_network.state_dict())\n",
    "target_action_value_network.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea89076e-a566-4063-9bbc-b02a3ba37718",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN_Agent(\n",
    "    model = action_value_network,\n",
    "    target_model = target_action_value_network,\n",
    "    device = device,\n",
    "    action_space = ACTION_SPACE,\n",
    "    replay_memory = replay_memory,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    min_expl = min_expl,\n",
    "    max_expl = max_expl,\n",
    "    expl_decay = expl_decay,\n",
    "    epoch_train_start = EPOCH_TRAIN_START,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    discount_rate = DISCOUNT_RATE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e42c8a69-0651-469b-bed6-b0f018915af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_reward = float(\"-inf\")\n",
    "train_save_path = \"/tmp/train_dqn.pt\"\n",
    "val_save_path = \"/tmp/val_dqn.pt\"\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_log = agent.train(train_env, epoch)\n",
    "    val_log = agent.validate(val_env)\n",
    "    \n",
    "    val_reward = val_log['episode_reward']\n",
    "    if val_reward > best_val_reward:\n",
    "        best_val_reward = val_reward\n",
    "        action_value_network.save(val_save_path)\n",
    "        mlflow.log_artifact(val_save_path)\n",
    "action_value_network.save(train_save_path)\n",
    "mlflow.log_artifact(train_save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51449f34-f77d-4a96-a643-3249230a16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_val_log = agent.validate(val_env, learn=True)\n",
    "test_log = agent.test(test_env, learn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78b0328c-46f7-42e4-8ce9-d701de217c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(torch.tensor([1,2,3]), '__iter__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cb1775-a795-4208-b354-280611aa595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e467d2d-35c7-4e03-8874-1452522687c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_log(log, epoch, mode):\n",
    "    for k, v in log.items():\n",
    "        if not hasattr(v, '__iter__'):\n",
    "            mlflow.log_metric(k, v, epoch=epoch)\n",
    "    \n",
    "    # Log losses\n",
    "    losses = log['losses'][-1]\n",
    "    mlflow.log_metric(f\"{mode}_loss\", losses.item())\n",
    "    \n",
    "    # Log action history\n",
    "    action_info = dict(pd.Series(log['episode_action_history']).value_counts())\n",
    "    for action, action_count in action_info.items():\n",
    "        mlflow.log_metric(f\"{mode}_action_{action}\", action_count, epoch=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364f163-673d-4459-9437-84d0a324bd70",
   "metadata": {},
   "source": [
    "Need to log models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaf08e54-aef5-40a9-b524-5d6db4214cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step_count': 14358,\n",
       " 'episode_reward': -3.05809000000153,\n",
       " 'episode_action_history': [0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  ...],\n",
       " 'losses': [5.94605016708374,\n",
       "  6.759302139282227,\n",
       "  6.476536750793457,\n",
       "  6.2199788093566895,\n",
       "  6.184223651885986,\n",
       "  5.556191921234131,\n",
       "  6.1062846183776855,\n",
       "  6.513092041015625,\n",
       "  6.021137237548828,\n",
       "  6.300362586975098,\n",
       "  5.78115177154541,\n",
       "  6.013006210327148,\n",
       "  6.846295356750488,\n",
       "  5.229162216186523,\n",
       "  6.393686294555664,\n",
       "  5.949185848236084,\n",
       "  6.042905807495117,\n",
       "  5.841012954711914,\n",
       "  5.279699325561523,\n",
       "  6.330587387084961,\n",
       "  5.596537113189697,\n",
       "  5.475211143493652,\n",
       "  6.528050422668457,\n",
       "  6.406833648681641,\n",
       "  5.13658332824707,\n",
       "  5.8968706130981445,\n",
       "  5.789820194244385,\n",
       "  5.616762161254883,\n",
       "  6.296520233154297,\n",
       "  6.807244300842285,\n",
       "  5.7266621589660645,\n",
       "  6.29307746887207,\n",
       "  5.834146499633789,\n",
       "  6.553353786468506,\n",
       "  5.987586498260498,\n",
       "  5.756430625915527,\n",
       "  6.914583683013916,\n",
       "  5.376672267913818,\n",
       "  5.885822296142578,\n",
       "  5.703330993652344,\n",
       "  5.372091293334961,\n",
       "  5.686781883239746,\n",
       "  17.457256317138672,\n",
       "  6.320681095123291,\n",
       "  6.34153938293457,\n",
       "  5.8583879470825195,\n",
       "  5.918546199798584,\n",
       "  6.27137565612793,\n",
       "  5.771320819854736,\n",
       "  6.039853096008301,\n",
       "  5.8494720458984375,\n",
       "  5.447330951690674,\n",
       "  5.938224792480469,\n",
       "  5.689235210418701,\n",
       "  5.275773048400879,\n",
       "  6.085908889770508,\n",
       "  5.379119873046875,\n",
       "  6.075679779052734,\n",
       "  5.867635250091553,\n",
       "  6.22749137878418,\n",
       "  6.270903587341309,\n",
       "  5.884389877319336,\n",
       "  5.022498607635498,\n",
       "  6.260857105255127,\n",
       "  5.869716644287109,\n",
       "  6.494946479797363,\n",
       "  5.728632926940918,\n",
       "  6.903878211975098,\n",
       "  5.905763149261475,\n",
       "  6.180463790893555,\n",
       "  6.314196586608887,\n",
       "  6.110576629638672,\n",
       "  6.192341327667236,\n",
       "  5.448894500732422,\n",
       "  5.337406158447266,\n",
       "  4.9380717277526855,\n",
       "  6.355682373046875,\n",
       "  6.239236354827881,\n",
       "  6.4297261238098145,\n",
       "  5.934340953826904,\n",
       "  7.369852066040039,\n",
       "  5.75663423538208,\n",
       "  5.480681896209717,\n",
       "  5.911219596862793,\n",
       "  6.316364288330078,\n",
       "  6.162053108215332,\n",
       "  6.230411529541016,\n",
       "  5.656548023223877,\n",
       "  5.941372871398926,\n",
       "  5.931593894958496,\n",
       "  6.049979209899902,\n",
       "  6.395744800567627,\n",
       "  6.218268394470215,\n",
       "  5.1243720054626465,\n",
       "  5.606452941894531,\n",
       "  5.454464435577393,\n",
       "  6.674107074737549,\n",
       "  5.690434455871582,\n",
       "  6.087440490722656,\n",
       "  6.639711380004883,\n",
       "  6.083791732788086,\n",
       "  5.172000885009766,\n",
       "  5.696716785430908,\n",
       "  6.342904090881348,\n",
       "  6.552979946136475,\n",
       "  5.933598518371582,\n",
       "  5.653768539428711,\n",
       "  5.964099884033203,\n",
       "  6.1327972412109375,\n",
       "  5.491576194763184,\n",
       "  6.064745903015137,\n",
       "  5.345083236694336,\n",
       "  5.580737590789795,\n",
       "  5.741025924682617,\n",
       "  5.788155555725098,\n",
       "  7.073054790496826,\n",
       "  6.733275413513184,\n",
       "  6.080985069274902,\n",
       "  8.756512641906738,\n",
       "  6.087477684020996,\n",
       "  6.558937072753906,\n",
       "  6.085099220275879,\n",
       "  6.071373462677002,\n",
       "  6.695852756500244,\n",
       "  5.656081676483154,\n",
       "  5.593050956726074,\n",
       "  6.000918388366699,\n",
       "  6.477521896362305,\n",
       "  6.83477258682251,\n",
       "  6.079687118530273,\n",
       "  5.6941118240356445,\n",
       "  5.238073825836182,\n",
       "  6.23942232131958,\n",
       "  6.178350448608398,\n",
       "  5.9168291091918945,\n",
       "  5.838053226470947,\n",
       "  5.94696569442749,\n",
       "  5.3555779457092285,\n",
       "  5.178197860717773,\n",
       "  5.216315269470215,\n",
       "  5.981770992279053,\n",
       "  5.441640853881836,\n",
       "  6.447265625,\n",
       "  5.776227951049805,\n",
       "  6.4047698974609375,\n",
       "  5.783437728881836,\n",
       "  5.609284400939941,\n",
       "  6.422922134399414,\n",
       "  6.2494330406188965,\n",
       "  5.931162357330322,\n",
       "  6.122077941894531,\n",
       "  5.712167739868164,\n",
       "  5.629208087921143,\n",
       "  8.946313858032227,\n",
       "  5.913741111755371,\n",
       "  5.85154914855957,\n",
       "  6.35248327255249,\n",
       "  5.920522689819336,\n",
       "  5.963693618774414,\n",
       "  8.693214416503906,\n",
       "  5.783781051635742,\n",
       "  5.3276824951171875,\n",
       "  5.464826583862305,\n",
       "  5.832493305206299,\n",
       "  5.618376731872559,\n",
       "  5.876929759979248,\n",
       "  5.871834754943848,\n",
       "  6.373117446899414,\n",
       "  5.906661033630371,\n",
       "  6.301059246063232,\n",
       "  5.863405227661133,\n",
       "  5.1816887855529785,\n",
       "  5.844961643218994,\n",
       "  6.034984588623047,\n",
       "  6.3898468017578125,\n",
       "  5.710296154022217,\n",
       "  6.215561866760254,\n",
       "  5.762869834899902,\n",
       "  6.560929298400879,\n",
       "  6.165335655212402,\n",
       "  5.532664775848389,\n",
       "  6.034827709197998,\n",
       "  5.87087869644165,\n",
       "  6.142777442932129,\n",
       "  6.320253849029541,\n",
       "  5.942498207092285,\n",
       "  6.739530086517334,\n",
       "  5.103551864624023,\n",
       "  5.958354949951172,\n",
       "  5.199122905731201,\n",
       "  6.149779319763184,\n",
       "  6.312759876251221,\n",
       "  5.735538482666016,\n",
       "  5.610261917114258,\n",
       "  6.088319778442383,\n",
       "  5.829972743988037,\n",
       "  5.891448974609375,\n",
       "  5.988420486450195,\n",
       "  5.89833927154541,\n",
       "  5.5336833000183105,\n",
       "  6.4010491371154785,\n",
       "  5.303572654724121,\n",
       "  5.866277694702148,\n",
       "  6.2070465087890625,\n",
       "  5.58009147644043,\n",
       "  5.7127790451049805,\n",
       "  6.324516296386719,\n",
       "  5.758234024047852,\n",
       "  6.685390472412109,\n",
       "  6.450004577636719,\n",
       "  5.973021030426025,\n",
       "  5.238483905792236,\n",
       "  5.859699249267578,\n",
       "  5.92136287689209,\n",
       "  5.750227928161621,\n",
       "  5.777235984802246,\n",
       "  5.320613384246826,\n",
       "  5.783078193664551,\n",
       "  6.080724239349365,\n",
       "  5.520937919616699,\n",
       "  5.17550802230835,\n",
       "  5.945985794067383,\n",
       "  5.8792243003845215,\n",
       "  6.521830081939697,\n",
       "  6.149501800537109,\n",
       "  5.482685565948486,\n",
       "  5.893614768981934,\n",
       "  5.5535173416137695,\n",
       "  5.754565238952637,\n",
       "  5.608706474304199,\n",
       "  6.420843124389648,\n",
       "  5.4363861083984375,\n",
       "  6.793668270111084,\n",
       "  5.759423732757568,\n",
       "  5.8948493003845215,\n",
       "  5.974274635314941,\n",
       "  6.215310096740723,\n",
       "  6.728756427764893,\n",
       "  5.29657506942749,\n",
       "  6.516479969024658,\n",
       "  6.002938270568848,\n",
       "  5.348908424377441,\n",
       "  6.198612213134766,\n",
       "  5.141140460968018,\n",
       "  5.6248650550842285,\n",
       "  5.302677154541016,\n",
       "  5.541505813598633,\n",
       "  5.683681964874268,\n",
       "  5.846808433532715,\n",
       "  6.024659156799316,\n",
       "  5.705437660217285,\n",
       "  7.051972389221191,\n",
       "  5.273341178894043,\n",
       "  5.829118728637695,\n",
       "  6.006841659545898,\n",
       "  6.489279747009277,\n",
       "  5.868802547454834,\n",
       "  6.360071182250977,\n",
       "  6.039868354797363,\n",
       "  5.924943447113037,\n",
       "  6.008586406707764,\n",
       "  6.4953413009643555,\n",
       "  5.3799943923950195,\n",
       "  5.859186172485352,\n",
       "  6.381063461303711,\n",
       "  7.212074279785156,\n",
       "  6.093247413635254,\n",
       "  5.53923225402832,\n",
       "  5.5697126388549805,\n",
       "  5.950298309326172,\n",
       "  6.408048629760742,\n",
       "  5.822845458984375,\n",
       "  5.479862213134766,\n",
       "  5.951696395874023,\n",
       "  4.982810974121094,\n",
       "  5.961012840270996,\n",
       "  5.727153778076172,\n",
       "  6.13344669342041,\n",
       "  5.977117538452148,\n",
       "  4.768927097320557,\n",
       "  5.901351451873779,\n",
       "  6.609750747680664,\n",
       "  5.268402576446533,\n",
       "  5.757876396179199,\n",
       "  5.859077453613281,\n",
       "  6.616163730621338,\n",
       "  6.364514350891113,\n",
       "  6.733268737792969,\n",
       "  6.251471996307373,\n",
       "  5.784638404846191,\n",
       "  6.216363430023193,\n",
       "  6.253570556640625,\n",
       "  5.742146015167236,\n",
       "  6.2127251625061035,\n",
       "  5.788226127624512,\n",
       "  5.634731292724609,\n",
       "  5.640522003173828,\n",
       "  5.975668907165527,\n",
       "  5.594511985778809,\n",
       "  5.901521682739258,\n",
       "  5.809238433837891,\n",
       "  6.656063079833984,\n",
       "  5.545623779296875,\n",
       "  6.164102554321289,\n",
       "  6.073897838592529,\n",
       "  5.37125301361084,\n",
       "  8.534125328063965,\n",
       "  6.24566125869751,\n",
       "  6.2657856941223145,\n",
       "  5.242711067199707,\n",
       "  8.05968952178955,\n",
       "  6.287049293518066,\n",
       "  5.8581461906433105,\n",
       "  5.874608039855957,\n",
       "  5.036859035491943,\n",
       "  5.88767147064209,\n",
       "  5.73429012298584,\n",
       "  5.959900856018066,\n",
       "  5.654001712799072,\n",
       "  5.670839309692383,\n",
       "  5.629159450531006,\n",
       "  5.898681163787842,\n",
       "  6.161831855773926,\n",
       "  6.151910781860352,\n",
       "  5.794425010681152,\n",
       "  6.297069549560547,\n",
       "  7.542269706726074,\n",
       "  6.3902082443237305,\n",
       "  5.819829940795898,\n",
       "  5.751265525817871,\n",
       "  5.565164566040039,\n",
       "  5.900042533874512,\n",
       "  5.499322891235352,\n",
       "  5.857239723205566,\n",
       "  6.1010284423828125,\n",
       "  5.69412088394165,\n",
       "  5.4361371994018555,\n",
       "  5.241887092590332,\n",
       "  6.292787551879883,\n",
       "  5.407179832458496,\n",
       "  6.774333953857422,\n",
       "  5.013918876647949,\n",
       "  6.518411159515381,\n",
       "  5.822162628173828,\n",
       "  5.497124671936035,\n",
       "  5.548949718475342,\n",
       "  5.733764171600342,\n",
       "  6.457135200500488,\n",
       "  6.2002410888671875,\n",
       "  5.739792823791504,\n",
       "  5.444718360900879,\n",
       "  5.336569786071777,\n",
       "  6.160929203033447,\n",
       "  5.39597749710083,\n",
       "  5.574195861816406,\n",
       "  6.144890785217285,\n",
       "  5.552684307098389,\n",
       "  9.530359268188477,\n",
       "  5.319571495056152,\n",
       "  6.1094794273376465,\n",
       "  6.517833709716797,\n",
       "  5.781423568725586,\n",
       "  5.800713539123535,\n",
       "  5.546064376831055,\n",
       "  6.888449668884277,\n",
       "  5.514418601989746,\n",
       "  8.094625473022461,\n",
       "  6.310573577880859,\n",
       "  6.219871520996094,\n",
       "  5.745450973510742,\n",
       "  5.899445056915283,\n",
       "  6.161728858947754,\n",
       "  6.083737850189209,\n",
       "  4.938425064086914,\n",
       "  5.510396480560303,\n",
       "  8.336122512817383,\n",
       "  9.029607772827148,\n",
       "  5.976916313171387,\n",
       "  5.710245132446289,\n",
       "  6.236214637756348,\n",
       "  6.181472301483154,\n",
       "  5.274911880493164,\n",
       "  6.145050048828125,\n",
       "  5.75718879699707,\n",
       "  5.4608564376831055,\n",
       "  5.908604145050049,\n",
       "  6.0684428215026855,\n",
       "  5.483981132507324,\n",
       "  6.048907279968262,\n",
       "  6.150053024291992,\n",
       "  6.321535110473633,\n",
       "  6.629412651062012,\n",
       "  5.374284267425537,\n",
       "  6.655340194702148,\n",
       "  6.171082973480225,\n",
       "  5.144503116607666,\n",
       "  5.560437202453613,\n",
       "  5.759690284729004,\n",
       "  5.338499546051025,\n",
       "  6.3359055519104,\n",
       "  5.514605522155762,\n",
       "  5.255921363830566,\n",
       "  6.534507751464844,\n",
       "  6.2955169677734375,\n",
       "  6.217487335205078,\n",
       "  5.720002174377441,\n",
       "  5.814336776733398,\n",
       "  6.154916763305664,\n",
       "  5.650031089782715,\n",
       "  5.874598026275635,\n",
       "  6.173844814300537,\n",
       "  6.135191917419434,\n",
       "  4.909938812255859,\n",
       "  6.1087846755981445,\n",
       "  5.985891819000244,\n",
       "  6.757035255432129,\n",
       "  6.3757429122924805,\n",
       "  5.431631088256836,\n",
       "  5.586295127868652,\n",
       "  6.327500820159912,\n",
       "  6.025084972381592,\n",
       "  5.989041328430176,\n",
       "  5.94329833984375,\n",
       "  5.808485507965088,\n",
       "  5.355438709259033,\n",
       "  5.809022903442383,\n",
       "  7.09371280670166,\n",
       "  5.5791120529174805,\n",
       "  5.762667179107666,\n",
       "  5.732936382293701,\n",
       "  6.347507953643799,\n",
       "  5.832330703735352,\n",
       "  5.9665985107421875,\n",
       "  5.816251277923584,\n",
       "  5.600182056427002,\n",
       "  6.086027145385742,\n",
       "  5.605587959289551,\n",
       "  6.040221214294434,\n",
       "  5.774247169494629,\n",
       "  5.853017807006836,\n",
       "  5.626824855804443,\n",
       "  5.676241397857666,\n",
       "  5.454398155212402,\n",
       "  6.184892177581787,\n",
       "  5.680440425872803,\n",
       "  5.759037017822266,\n",
       "  5.925679683685303,\n",
       "  5.816808700561523,\n",
       "  6.574190139770508,\n",
       "  6.613825798034668,\n",
       "  6.170304298400879,\n",
       "  6.135244846343994,\n",
       "  5.6533403396606445,\n",
       "  5.705442428588867,\n",
       "  6.4493608474731445,\n",
       "  5.726780891418457,\n",
       "  5.509982109069824,\n",
       "  6.114169120788574,\n",
       "  5.535140037536621,\n",
       "  5.331993103027344,\n",
       "  6.269750595092773,\n",
       "  6.007559776306152,\n",
       "  5.70149040222168,\n",
       "  5.602136135101318,\n",
       "  5.677481651306152,\n",
       "  5.723172187805176,\n",
       "  6.151774883270264,\n",
       "  5.32334041595459,\n",
       "  6.388204574584961,\n",
       "  6.395543575286865,\n",
       "  5.784759521484375,\n",
       "  6.4996490478515625,\n",
       "  5.29183292388916,\n",
       "  6.418177604675293,\n",
       "  5.667830944061279,\n",
       "  5.811574935913086,\n",
       "  5.625663757324219,\n",
       "  6.084358215332031,\n",
       "  5.559614181518555,\n",
       "  5.236820697784424,\n",
       "  6.112288475036621,\n",
       "  5.332729816436768,\n",
       "  6.256468772888184,\n",
       "  6.2477569580078125,\n",
       "  6.83861780166626,\n",
       "  5.742786407470703,\n",
       "  6.053609848022461,\n",
       "  5.562582969665527,\n",
       "  6.284002304077148,\n",
       "  5.792784690856934,\n",
       "  6.325767993927002,\n",
       "  5.9649338722229,\n",
       "  5.388835430145264,\n",
       "  5.623762130737305,\n",
       "  6.815914154052734,\n",
       "  6.066451072692871,\n",
       "  6.29256534576416,\n",
       "  5.680161952972412,\n",
       "  6.050489902496338,\n",
       "  5.259890556335449,\n",
       "  5.210501194000244,\n",
       "  6.216670036315918,\n",
       "  5.585099220275879,\n",
       "  5.901052951812744,\n",
       "  6.202279567718506,\n",
       "  6.4481048583984375,\n",
       "  5.905843734741211,\n",
       "  5.525348663330078,\n",
       "  5.889953136444092,\n",
       "  5.791136741638184,\n",
       "  5.940130233764648,\n",
       "  6.237937927246094,\n",
       "  5.90158224105835,\n",
       "  6.094987869262695,\n",
       "  5.329738616943359,\n",
       "  6.03838586807251,\n",
       "  4.6366376876831055,\n",
       "  5.55218505859375,\n",
       "  5.812953948974609,\n",
       "  6.087957859039307,\n",
       "  5.493158340454102,\n",
       "  5.818832874298096,\n",
       "  6.20396089553833,\n",
       "  5.886472702026367,\n",
       "  5.568791389465332,\n",
       "  6.510550498962402,\n",
       "  5.463269233703613,\n",
       "  6.277068614959717,\n",
       "  18.042728424072266,\n",
       "  5.689505100250244,\n",
       "  5.987110614776611,\n",
       "  5.890472412109375,\n",
       "  6.6520867347717285,\n",
       "  5.697088241577148,\n",
       "  6.013005256652832,\n",
       "  6.440094470977783,\n",
       "  5.4038872718811035,\n",
       "  6.468461990356445,\n",
       "  6.156905174255371,\n",
       "  5.810441017150879,\n",
       "  6.305804252624512,\n",
       "  6.111930847167969,\n",
       "  6.053607940673828,\n",
       "  5.90792179107666,\n",
       "  4.738202095031738,\n",
       "  6.065122604370117,\n",
       "  5.418172836303711,\n",
       "  5.571841716766357,\n",
       "  6.016529083251953,\n",
       "  6.552527904510498,\n",
       "  6.773810863494873,\n",
       "  5.522668838500977,\n",
       "  6.916650772094727,\n",
       "  6.435183525085449,\n",
       "  5.4337944984436035,\n",
       "  5.542435646057129,\n",
       "  6.595324516296387,\n",
       "  5.625694274902344,\n",
       "  6.2436370849609375,\n",
       "  5.697510242462158,\n",
       "  5.857715129852295,\n",
       "  5.711400985717773,\n",
       "  5.333056449890137,\n",
       "  5.606900215148926,\n",
       "  5.4014410972595215,\n",
       "  5.285553932189941,\n",
       "  6.392889976501465,\n",
       "  5.601799964904785,\n",
       "  6.264457702636719,\n",
       "  6.309487819671631,\n",
       "  6.60506534576416,\n",
       "  5.432498931884766,\n",
       "  6.446561813354492,\n",
       "  5.692379951477051,\n",
       "  5.238221168518066,\n",
       "  5.400137901306152,\n",
       "  5.306491851806641,\n",
       "  5.412173748016357,\n",
       "  5.191679000854492,\n",
       "  5.603981971740723,\n",
       "  6.100584030151367,\n",
       "  6.41309928894043,\n",
       "  5.812529563903809,\n",
       "  5.3297882080078125,\n",
       "  6.212677001953125,\n",
       "  5.581763744354248,\n",
       "  5.824882507324219,\n",
       "  6.2595906257629395,\n",
       "  5.738383769989014,\n",
       "  8.66634750366211,\n",
       "  5.970037937164307,\n",
       "  5.8396525382995605,\n",
       "  5.674978256225586,\n",
       "  6.058727264404297,\n",
       "  5.729979515075684,\n",
       "  5.845912456512451,\n",
       "  5.247130393981934,\n",
       "  5.990110397338867,\n",
       "  6.71424674987793,\n",
       "  5.65211296081543,\n",
       "  5.970267295837402,\n",
       "  5.910712242126465,\n",
       "  5.246189117431641,\n",
       "  5.909355640411377,\n",
       "  5.996859550476074,\n",
       "  6.16132116317749,\n",
       "  6.277923583984375,\n",
       "  5.882205486297607,\n",
       "  5.434510231018066,\n",
       "  5.648311614990234,\n",
       "  5.774296760559082,\n",
       "  6.323099136352539,\n",
       "  6.018656253814697,\n",
       "  5.904023170471191,\n",
       "  6.2083353996276855,\n",
       "  5.663641929626465,\n",
       "  8.970010757446289,\n",
       "  5.962411403656006,\n",
       "  5.612713813781738,\n",
       "  5.495143890380859,\n",
       "  7.1609649658203125,\n",
       "  5.76740837097168,\n",
       "  5.1566267013549805,\n",
       "  6.121577739715576,\n",
       "  5.697897911071777,\n",
       "  5.479191780090332,\n",
       "  5.03116512298584,\n",
       "  5.783257484436035,\n",
       "  6.252121448516846,\n",
       "  5.170702934265137,\n",
       "  5.481400489807129,\n",
       "  6.547780513763428,\n",
       "  5.821461200714111,\n",
       "  6.166384220123291,\n",
       "  6.0061869621276855,\n",
       "  5.258678436279297,\n",
       "  5.757579326629639,\n",
       "  5.221070289611816,\n",
       "  5.5455145835876465,\n",
       "  5.949489593505859,\n",
       "  5.175355911254883,\n",
       "  5.392220497131348,\n",
       "  5.280585289001465,\n",
       "  5.872334957122803,\n",
       "  6.192381858825684,\n",
       "  6.462371826171875,\n",
       "  5.7091827392578125,\n",
       "  5.161028861999512,\n",
       "  5.764230251312256,\n",
       "  6.84073543548584,\n",
       "  5.606156349182129,\n",
       "  5.7747955322265625,\n",
       "  5.898606300354004,\n",
       "  5.636807918548584,\n",
       "  5.147601127624512,\n",
       "  5.555548667907715,\n",
       "  5.760292053222656,\n",
       "  5.584726810455322,\n",
       "  6.124216079711914,\n",
       "  5.773414611816406,\n",
       "  5.71778678894043,\n",
       "  5.766805648803711,\n",
       "  5.481122970581055,\n",
       "  6.0938496589660645,\n",
       "  6.234640121459961,\n",
       "  5.788463592529297,\n",
       "  5.371194839477539,\n",
       "  5.251398086547852,\n",
       "  5.774292945861816,\n",
       "  5.828324794769287,\n",
       "  5.9556450843811035,\n",
       "  5.305769443511963,\n",
       "  5.8180131912231445,\n",
       "  5.674389839172363,\n",
       "  5.9533185958862305,\n",
       "  5.787867069244385,\n",
       "  6.1382904052734375,\n",
       "  5.938012599945068,\n",
       "  6.131497383117676,\n",
       "  5.976437568664551,\n",
       "  5.458346366882324,\n",
       "  8.961719512939453,\n",
       "  6.004819869995117,\n",
       "  5.739642143249512,\n",
       "  6.514138221740723,\n",
       "  5.220354080200195,\n",
       "  6.213688850402832,\n",
       "  5.588666915893555,\n",
       "  6.605817794799805,\n",
       "  5.796760559082031,\n",
       "  6.102100372314453,\n",
       "  5.4878950119018555,\n",
       "  5.877708435058594,\n",
       "  5.871359348297119,\n",
       "  5.453519821166992,\n",
       "  5.298613548278809,\n",
       "  5.428738594055176,\n",
       "  7.207807540893555,\n",
       "  5.836715221405029,\n",
       "  5.874732971191406,\n",
       "  5.5178632736206055,\n",
       "  5.834506511688232,\n",
       "  6.015438079833984,\n",
       "  6.405052185058594,\n",
       "  6.167814254760742,\n",
       "  6.059447765350342,\n",
       "  5.574217319488525,\n",
       "  5.778775691986084,\n",
       "  6.070623874664307,\n",
       "  6.244045257568359,\n",
       "  6.356808662414551,\n",
       "  5.694777011871338,\n",
       "  5.859171390533447,\n",
       "  6.157292366027832,\n",
       "  5.3678998947143555,\n",
       "  5.667336463928223,\n",
       "  6.21168327331543,\n",
       "  6.173217296600342,\n",
       "  5.481378555297852,\n",
       "  5.997376441955566,\n",
       "  5.673007965087891,\n",
       "  5.056617736816406,\n",
       "  5.679687976837158,\n",
       "  5.4945573806762695,\n",
       "  5.997303485870361,\n",
       "  5.900609016418457,\n",
       "  6.293056964874268,\n",
       "  6.031773090362549,\n",
       "  5.83914852142334,\n",
       "  6.049289703369141,\n",
       "  5.761703968048096,\n",
       "  5.387231349945068,\n",
       "  5.4832072257995605,\n",
       "  5.737295150756836,\n",
       "  5.813350677490234,\n",
       "  5.815886497497559,\n",
       "  5.980928421020508,\n",
       "  5.314752578735352,\n",
       "  5.3124589920043945,\n",
       "  6.894176483154297,\n",
       "  6.5794806480407715,\n",
       "  4.6590728759765625,\n",
       "  17.34255599975586,\n",
       "  5.968755722045898,\n",
       "  6.381771564483643,\n",
       "  5.607916831970215,\n",
       "  5.727060317993164,\n",
       "  6.454782009124756,\n",
       "  5.778596878051758,\n",
       "  6.371634006500244,\n",
       "  5.773353576660156,\n",
       "  5.066628456115723,\n",
       "  6.893223285675049,\n",
       "  6.304474353790283,\n",
       "  5.91162109375,\n",
       "  5.609029769897461,\n",
       "  5.536212921142578,\n",
       "  5.959190368652344,\n",
       "  6.043391704559326,\n",
       "  5.523245811462402,\n",
       "  5.76066255569458,\n",
       "  6.285325050354004,\n",
       "  5.703851699829102,\n",
       "  5.328225135803223,\n",
       "  5.842057228088379,\n",
       "  6.112524032592773,\n",
       "  6.350826263427734,\n",
       "  5.192203521728516,\n",
       "  5.819281101226807,\n",
       "  5.39360237121582,\n",
       "  5.184904098510742,\n",
       "  6.090008735656738,\n",
       "  6.344878196716309,\n",
       "  5.573703765869141,\n",
       "  6.051939010620117,\n",
       "  5.961798667907715,\n",
       "  5.840456008911133,\n",
       "  6.464317321777344,\n",
       "  5.760570526123047,\n",
       "  5.564218521118164,\n",
       "  5.753400802612305,\n",
       "  5.7977519035339355,\n",
       "  5.738829612731934,\n",
       "  6.0997185707092285,\n",
       "  5.258769989013672,\n",
       "  5.639857769012451,\n",
       "  6.32020902633667,\n",
       "  5.7743682861328125,\n",
       "  9.41808032989502,\n",
       "  5.673466205596924,\n",
       "  6.307232856750488,\n",
       "  5.368808746337891,\n",
       "  5.98166036605835,\n",
       "  6.054751396179199,\n",
       "  5.766565322875977,\n",
       "  5.891300201416016,\n",
       "  5.548805236816406,\n",
       "  5.237096786499023,\n",
       "  6.051783084869385,\n",
       "  6.07185173034668,\n",
       "  6.13542366027832,\n",
       "  5.8569655418396,\n",
       "  6.030981540679932,\n",
       "  5.590570449829102,\n",
       "  5.665595054626465,\n",
       "  6.7898406982421875,\n",
       "  6.383831977844238,\n",
       "  6.272275447845459,\n",
       "  5.026488780975342,\n",
       "  6.26634407043457,\n",
       "  5.475332260131836,\n",
       "  6.0478315353393555,\n",
       "  5.8601226806640625,\n",
       "  5.357656955718994,\n",
       "  6.506687164306641,\n",
       "  5.645944595336914,\n",
       "  6.18803596496582,\n",
       "  5.56194543838501,\n",
       "  6.763215065002441,\n",
       "  6.346279144287109,\n",
       "  5.360407829284668,\n",
       "  6.079023361206055,\n",
       "  4.935070514678955,\n",
       "  5.602413177490234,\n",
       "  5.375944137573242,\n",
       "  6.128450393676758,\n",
       "  6.194282054901123,\n",
       "  8.960259437561035,\n",
       "  5.212174415588379,\n",
       "  18.526704788208008,\n",
       "  5.719069004058838,\n",
       "  6.1226582527160645,\n",
       "  6.200046539306641,\n",
       "  5.2568159103393555,\n",
       "  6.130420207977295,\n",
       "  6.195920944213867,\n",
       "  6.3327226638793945,\n",
       "  5.5693359375,\n",
       "  5.153470993041992,\n",
       "  5.3494439125061035,\n",
       "  6.110857009887695,\n",
       "  6.013772964477539,\n",
       "  5.030796051025391,\n",
       "  5.751019477844238,\n",
       "  6.09012508392334,\n",
       "  5.506312370300293,\n",
       "  5.430624961853027,\n",
       "  5.761115550994873,\n",
       "  6.148612022399902,\n",
       "  6.045459747314453,\n",
       "  5.157329082489014,\n",
       "  5.327947616577148,\n",
       "  5.409997940063477,\n",
       "  6.104878902435303,\n",
       "  6.379729270935059,\n",
       "  6.065702438354492,\n",
       "  5.537238597869873,\n",
       "  5.7528910636901855,\n",
       "  5.553376197814941,\n",
       "  5.238129138946533,\n",
       "  5.141445159912109,\n",
       "  6.403171539306641,\n",
       "  5.171538352966309,\n",
       "  6.355061054229736,\n",
       "  5.700741767883301,\n",
       "  4.794087886810303,\n",
       "  5.7462873458862305,\n",
       "  5.2233123779296875,\n",
       "  7.053640365600586,\n",
       "  5.297214984893799,\n",
       "  5.930861473083496,\n",
       "  5.948798179626465,\n",
       "  5.512255668640137,\n",
       "  5.380834579467773,\n",
       "  6.287331581115723,\n",
       "  5.0959272384643555,\n",
       "  5.5683913230896,\n",
       "  5.722530364990234,\n",
       "  5.878979206085205,\n",
       "  6.353520393371582,\n",
       "  5.3150482177734375,\n",
       "  5.778476715087891,\n",
       "  6.205351829528809,\n",
       "  5.796494483947754,\n",
       "  6.267984390258789,\n",
       "  6.477476119995117,\n",
       "  5.560992240905762,\n",
       "  6.5248003005981445,\n",
       "  5.243993759155273,\n",
       "  6.143253803253174,\n",
       "  5.299741744995117,\n",
       "  5.269999980926514,\n",
       "  5.220424175262451,\n",
       "  5.538711071014404,\n",
       "  6.180121421813965,\n",
       "  5.3286943435668945,\n",
       "  6.3693742752075195,\n",
       "  5.5037946701049805,\n",
       "  5.434979438781738,\n",
       "  6.008054733276367,\n",
       "  5.46460485458374,\n",
       "  5.679986000061035,\n",
       "  5.340712547302246,\n",
       "  5.543902397155762,\n",
       "  5.767913818359375,\n",
       "  6.5337982177734375,\n",
       "  5.022607803344727,\n",
       "  6.180765151977539,\n",
       "  5.258081436157227,\n",
       "  5.429586887359619,\n",
       "  6.110691070556641,\n",
       "  5.694698333740234,\n",
       "  6.183321952819824,\n",
       "  5.269020080566406,\n",
       "  6.045339584350586,\n",
       "  5.741148948669434,\n",
       "  5.84993839263916,\n",
       "  6.184587478637695,\n",
       "  5.940523624420166,\n",
       "  5.744555473327637,\n",
       "  6.077067852020264,\n",
       "  5.477118492126465,\n",
       "  5.754980087280273,\n",
       "  5.764909744262695,\n",
       "  5.63881254196167,\n",
       "  6.287557601928711,\n",
       "  5.609355926513672,\n",
       "  5.471476078033447,\n",
       "  5.228767395019531,\n",
       "  5.904572486877441,\n",
       "  5.612661361694336,\n",
       "  5.9862260818481445,\n",
       "  5.893978595733643,\n",
       "  6.040818214416504,\n",
       "  5.938162326812744,\n",
       "  6.177063941955566,\n",
       "  5.82608699798584,\n",
       "  6.049257278442383,\n",
       "  5.562211513519287,\n",
       "  6.0572333335876465,\n",
       "  5.336484432220459,\n",
       "  6.172064781188965,\n",
       "  6.1580810546875,\n",
       "  5.511425495147705,\n",
       "  6.242992401123047,\n",
       "  5.653873443603516,\n",
       "  9.047295570373535,\n",
       "  5.727865695953369,\n",
       "  5.047936916351318,\n",
       "  5.487246990203857,\n",
       "  6.025537490844727,\n",
       "  5.859597206115723,\n",
       "  5.6952714920043945,\n",
       "  5.608521461486816,\n",
       "  5.135375022888184,\n",
       "  6.081301689147949,\n",
       "  5.577946662902832,\n",
       "  5.452846050262451,\n",
       "  5.603689193725586,\n",
       "  6.149352073669434,\n",
       "  5.573850631713867,\n",
       "  6.2646803855896,\n",
       "  6.402918815612793,\n",
       "  7.105079174041748,\n",
       "  5.899188995361328,\n",
       "  5.757293224334717,\n",
       "  5.645493507385254,\n",
       "  5.697505950927734,\n",
       "  5.822644233703613,\n",
       "  5.199640274047852,\n",
       "  5.41196346282959,\n",
       "  5.0267720222473145,\n",
       "  6.081988334655762,\n",
       "  5.521628379821777,\n",
       "  6.308778762817383,\n",
       "  5.449010848999023,\n",
       "  5.427757740020752,\n",
       "  5.814940452575684,\n",
       "  5.977822303771973,\n",
       "  5.872529029846191,\n",
       "  5.889676570892334,\n",
       "  6.057976722717285,\n",
       "  5.499818801879883,\n",
       "  5.800204753875732,\n",
       "  6.012468338012695,\n",
       "  5.327597618103027,\n",
       "  5.640081405639648,\n",
       "  6.1909098625183105,\n",
       "  6.324295997619629,\n",
       "  5.389512538909912,\n",
       "  5.571345329284668,\n",
       "  6.181590557098389,\n",
       "  5.0548577308654785,\n",
       "  6.245077133178711,\n",
       "  4.655925750732422,\n",
       "  5.513415336608887,\n",
       "  18.389751434326172,\n",
       "  5.925553798675537,\n",
       "  5.507080554962158,\n",
       "  5.751902103424072,\n",
       "  ...]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f56d1-f7d4-4db0-b25c-bcb9834fb78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc7266-c3a0-46df-a4a6-b9c82eaf6fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354df14-ff1e-47b6-8279-5835b6226d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5f94c3-62f1-4324-b1f2-d45aa382ca68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc2b8a-e559-4efb-8d02-838188819e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0319c09-b054-4bd8-ab28-2d0ec2bb2773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc810c7-a6cc-497f-838d-8f62d1b6ad2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237cab3-eeb8-4a79-aa20-4a3e52741768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727f6fc-73e2-4737-bc94-96da0ba98b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e7943e7a-87b9-4575-b6a9-860d7572d6bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'value_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13630/2548687851.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mvalue_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mepisode_action_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepisode_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'value_network' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    value_network.train()\n",
    "    episode_action_history = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    state = X_train[0].clone()\n",
    "    bid, ask = bid_train[0], ask_train[0]\n",
    "\n",
    "    for i in range(1, X_train.shape[0]):\n",
    "        # Explore vs exploit\n",
    "        exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*(epoch - EPOCH_TRAIN_START))\n",
    "        if random.uniform(0, 1) > exploration_rate:\n",
    "            value_network(state).argmax()\n",
    "        else:\n",
    "            action = torch.tensor(random.sample(ACTION_SPACE, 1)).to(device) # Explore\n",
    "\n",
    "        # Environment step\n",
    "        next_state = X_train[i].clone()\n",
    "        next_state[..., -1] = action\n",
    "        next_bid = bid_train[i]\n",
    "        next_ask = ask_train[i]\n",
    "\n",
    "        reward = get_reward(state, bid, ask, next_state, next_bid, next_ask, use_midprice=USE_MIDPRICE, trading_fee=TRADING_FEE, neutral_penalty=NEUTRAL_PENALTY)\n",
    "        done = i == (X_train.shape[0] - 1)\n",
    "\n",
    "        # Saves\n",
    "        replay_memory.push(Experience(state, action, reward, next_state, done))\n",
    "        episode_action_history.append(action.item())\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        bid = next_bid\n",
    "        ask = next_ask\n",
    "\n",
    "        if replay_memory.can_provide_sample(BATCH_SIZE) and epoch > EPOCH_TRAIN_START:\n",
    "            states, actions, rewards, next_states, dones = replay_memory.sample(BATCH_SIZE)\n",
    "            states, actions, rewards, next_states, dones = states.to(device), actions.to(device), rewards.to(device), next_states.to(device), dones.to(device)\n",
    "            actions = actions.long()\n",
    "            dones = dones.float()\n",
    "\n",
    "            # Error calculation\n",
    "            action_targets = value_network(next_states).max(dim=-1)[1].unsqueeze(-1)\n",
    "            target_values = rewards + (1 - dones) * DISCOUNT_RATE * fixed_network(next_states).gather(-1, action_targets).squeeze(-1) # Double DQN\n",
    "            # target_values = rewards + (1 - dones) * DISCOUNT_RATE * q_fn_frozen(next_states).max(dim=-1)[0] # DQN\n",
    "\n",
    "            current_values = value_network(states).gather(-1, actions.unsqueeze(-1)).squeeze(1)\n",
    "\n",
    "            # Loss\n",
    "            loss = mse_fn(current_values, target_values.detach())\n",
    "            value_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            value_optimizer.step()\n",
    "\n",
    "    reward_history.append(episode_reward) ######## LOG THE REWARD\n",
    "    mlflow.log_metric(\"episode_reward\", episode_reward, step=epoch)\n",
    "\n",
    "    if replay_memory.can_provide_sample(BATCH_SIZE) and i % FROZEN_UPDATE_INTERVAL == 0 and epoch > EPOCH_TRAIN_START:\n",
    "        fixed_network.load_state_dict(value_network.state_dict())\n",
    "        fixed_network.eval()\n",
    "\n",
    "    if replay_memory.can_provide_sample(BATCH_SIZE) and epoch > EPOCH_TRAIN_START:\n",
    "            print(f\"Epoch: {epoch:>5,d}, Reward: {episode_reward:>10,.6f}, Value loss: {loss.item():>10,.6f}\", end=\"\\r\")\n",
    "            mlflow.log_metric(\"train_value_network_loss\", loss.item(), step=epoch)\n",
    "\n",
    "            # Save episode action information - Need to see if the agent is changing its portfolio regularly\n",
    "            for k, v in dict(pd.Series(episode_action_history).value_counts()).items():\n",
    "                mlflow.log_metric(f\"action_{k}\", v, step=epoch)\n",
    "\n",
    "    if (episode_reward > best_episode_reward):\n",
    "        best_episode_reward = episode_reward\n",
    "        torch.save(value_network.state_dict(), \"best_train_value_state_dict.pt\")\n",
    "        mlflow.log_artifact(\"best_train_value_state_dict.pt\")\n",
    "################\n",
    "    with torch.no_grad():\n",
    "        # Validation checks\n",
    "        value_network.eval()\n",
    "        val_state = X_val[0]\n",
    "        val_bid = bid_val[0]\n",
    "        val_ask = ask_val[0]\n",
    "        validation_reward = 0\n",
    "        validation_actions = []\n",
    "\n",
    "        for k in range(1, X_val.shape[0]):\n",
    "            val_action = value_network(val_state).argmax()\n",
    "\n",
    "            val_next_state = X_val[k]\n",
    "            val_next_state[..., -1] = val_action\n",
    "\n",
    "            val_next_bid = bid_val[k]\n",
    "            val_next_ask = ask_val[k]\n",
    "\n",
    "            reward = get_reward(val_state, val_bid, val_ask, val_next_state, val_next_bid, val_next_ask, neutral_penalty=0)\n",
    "            validation_reward += reward\n",
    "            validation_actions.append(val_action.item())\n",
    "            val_state = val_next_state\n",
    "            val_bid = val_next_bid\n",
    "            val_ask = val_next_ask\n",
    "\n",
    "        mlflow.log_metric(\"validation_episode_reward\", validation_reward, step=epoch)\n",
    "        for key, value in dict(pd.Series(validation_actions).value_counts()).items():\n",
    "            mlflow.log_metric(f\"validation_action_{key}\", value, step=epoch)\n",
    "\n",
    "        if validation_reward > best_episode_validation_reward:\n",
    "            best_episode_validation_reward = validation_reward\n",
    "            torch.save(value_network.state_dict(), \"best_val_value_state_dict.pt\")\n",
    "            mlflow.log_artifact(\"best_val_value_state_dict.pt\")\n",
    "        value_network.train()\n",
    "\n",
    "torch.save(value_network.state_dict(), \"last_train_value_state_dict.pt\")\n",
    "mlflow.log_artifact(\"last_train_value_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bdf3e2-2776-4f2d-a9d5-344397407d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad4ea0-6ba3-4b46-a3d1-fbcafa3cbd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56281de0-8424-4686-b1ef-bfb9aced9e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e1ec8-6d0e-4a86-8287-7e8b4533f121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    14, Reward: -29.872640, Value loss:   0.241062\r"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    for USE_MIDPRICE in [False, True]:\n",
    "        # Fetch data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, bid_train, bid_val, bid_test, ask_train, ask_val, ask_test = [torch.load(path) for path in dataset]\n",
    "        y_train, y_val, y_test = y_train.long(), y_val.long(), y_test.long()\n",
    "        INPUT_DIMS = X_train.shape[-1]\n",
    "        \n",
    "        value_network = ValueNetwork(input_dim = INPUT_DIMS, output_dim = OUTPUT_DIMS).to(device)\n",
    "        fixed_network = ValueNetwork(input_dim = INPUT_DIMS, output_dim = OUTPUT_DIMS).to(device)\n",
    "        fixed_network.load_state_dict(value_network.state_dict())\n",
    "        value_network.train()\n",
    "        fixed_network.eval()\n",
    "        \n",
    "        value_optimizer = optim.Adam(params=value_network.parameters(), lr=LEARNING_RATE)\n",
    "        mse_fn = nn.MSELoss()\n",
    "        \n",
    "        # Begin logging\n",
    "        with mlflow.start_run() as run:\n",
    "            mlflow.log_params({\n",
    "                'batch_size': BATCH_SIZE,\n",
    "                'n_epochs': N_EPOCHS,\n",
    "                'train_start_epoch': EPOCH_TRAIN_START,\n",
    "                'learning_rate': LEARNING_RATE,\n",
    "                'replay_memory_size': REPLAY_MEMORY_SIZE,\n",
    "                'discount_rate': DISCOUNT_RATE,\n",
    "                'frozen_update_interval': FROZEN_UPDATE_INTERVAL,\n",
    "                'use_midprice': USE_MIDPRICE,\n",
    "                'trading_fee': TRADING_FEE,\n",
    "                'neutral_penalty': NEUTRAL_PENALTY,\n",
    "                'market_open': MARKET_OPEN,\n",
    "                'market_close': MARKET_CLOSE,\n",
    "                'granularity': GRANULARITY,\n",
    "                'month': MONTH,\n",
    "                'datascope': dataset[0].split(\"/\")[2],\n",
    "                'device': device,\n",
    "                'input_dims': INPUT_DIMS,\n",
    "                'critic_loss_fn': 'MSE',\n",
    "                'network_optimizer_fns': 'Adam',\n",
    "                'pretrain_loss_fn': 'CrossEntropyLoss',\n",
    "                'policy_network': str(value_network.linear_stack).replace(\"in_features=\", \"\").replace(\"out_features=\", \"\").replace(\" \", \"\").replace(\"\\n\", \"\").replace(\",bias=True\", \"\"),\n",
    "                'min_exploration_rate': min_exploration_rate,\n",
    "                'max_exploration_rate': max_exploration_rate,\n",
    "                'exploration_decay_rate': exploration_decay_rate,\n",
    "            })\n",
    "\n",
    "            critic_loss_history = []\n",
    "            policy_loss_history = []\n",
    "            reward_history = []\n",
    "            replay_memory = ReplayMemory(REPLAY_MEMORY_SIZE) # 2**20 values\n",
    "            \n",
    "            best_episode_reward = float(\"-inf\")\n",
    "            best_episode_validation_reward = float(\"-inf\")\n",
    "\n",
    "            for epoch in range(N_EPOCHS):\n",
    "                value_network.train()\n",
    "                episode_action_history = []\n",
    "                done = False\n",
    "                episode_reward = 0\n",
    "                state = X_train[0].clone()\n",
    "                bid, ask = bid_train[0], ask_train[0]\n",
    "\n",
    "                for i in range(1, X_train.shape[0]):\n",
    "                    # Explore vs exploit\n",
    "                    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*(epoch - EPOCH_TRAIN_START))\n",
    "                    if random.uniform(0, 1) > exploration_rate:\n",
    "                        value_network(state).argmax()\n",
    "                    else:\n",
    "                        action = torch.tensor(random.sample(ACTION_SPACE, 1)).to(device) # Explore\n",
    "\n",
    "                    # Environment step\n",
    "                    next_state = X_train[i].clone()\n",
    "                    next_state[..., -1] = action\n",
    "                    next_bid = bid_train[i]\n",
    "                    next_ask = ask_train[i]\n",
    "\n",
    "                    reward = get_reward(state, bid, ask, next_state, next_bid, next_ask, use_midprice=USE_MIDPRICE, trading_fee=TRADING_FEE, neutral_penalty=NEUTRAL_PENALTY)\n",
    "                    done = i == (X_train.shape[0] - 1)\n",
    "\n",
    "                    # Saves\n",
    "                    replay_memory.push(Experience(state, action, reward, next_state, done))\n",
    "                    episode_action_history.append(action.item())\n",
    "                    episode_reward += reward\n",
    "                    state = next_state\n",
    "                    bid = next_bid\n",
    "                    ask = next_ask\n",
    "\n",
    "                    if replay_memory.can_provide_sample(BATCH_SIZE) and epoch > EPOCH_TRAIN_START:\n",
    "                        states, actions, rewards, next_states, dones = replay_memory.sample(BATCH_SIZE)\n",
    "                        states, actions, rewards, next_states, dones = states.to(device), actions.to(device), rewards.to(device), next_states.to(device), dones.to(device)\n",
    "                        actions = actions.long()\n",
    "                        dones = dones.float()\n",
    "\n",
    "                        # Error calculation\n",
    "                        action_targets = value_network(next_states).max(dim=-1)[1].unsqueeze(-1)\n",
    "                        target_values = rewards + (1 - dones) * DISCOUNT_RATE * fixed_network(next_states).gather(-1, action_targets).squeeze(-1) # Double DQN\n",
    "                        # target_values = rewards + (1 - dones) * DISCOUNT_RATE * q_fn_frozen(next_states).max(dim=-1)[0] # DQN\n",
    "\n",
    "                        current_values = value_network(states).gather(-1, actions.unsqueeze(-1)).squeeze(1)\n",
    "\n",
    "                        # Loss\n",
    "                        loss = mse_fn(current_values, target_values.detach())\n",
    "                        value_optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        value_optimizer.step()\n",
    "\n",
    "                reward_history.append(episode_reward) ######## LOG THE REWARD\n",
    "                mlflow.log_metric(\"episode_reward\", episode_reward, step=epoch)\n",
    "\n",
    "                if replay_memory.can_provide_sample(BATCH_SIZE) and i % FROZEN_UPDATE_INTERVAL == 0 and epoch > EPOCH_TRAIN_START:\n",
    "                    fixed_network.load_state_dict(value_network.state_dict())\n",
    "                    fixed_network.eval()\n",
    "\n",
    "                if replay_memory.can_provide_sample(BATCH_SIZE) and epoch > EPOCH_TRAIN_START:\n",
    "                        print(f\"Epoch: {epoch:>5,d}, Reward: {episode_reward:>10,.6f}, Value loss: {loss.item():>10,.6f}\", end=\"\\r\")\n",
    "                        mlflow.log_metric(\"train_value_network_loss\", loss.item(), step=epoch)\n",
    "                        \n",
    "                        # Save episode action information - Need to see if the agent is changing its portfolio regularly\n",
    "                        for k, v in dict(pd.Series(episode_action_history).value_counts()).items():\n",
    "                            mlflow.log_metric(f\"action_{k}\", v, step=epoch)\n",
    "                            \n",
    "                if (episode_reward > best_episode_reward):\n",
    "                    best_episode_reward = episode_reward\n",
    "                    torch.save(value_network.state_dict(), \"best_train_value_state_dict.pt\")\n",
    "                    mlflow.log_artifact(\"best_train_value_state_dict.pt\")\n",
    "################\n",
    "                with torch.no_grad():\n",
    "                    # Validation checks\n",
    "                    value_network.eval()\n",
    "                    val_state = X_val[0]\n",
    "                    val_bid = bid_val[0]\n",
    "                    val_ask = ask_val[0]\n",
    "                    validation_reward = 0\n",
    "                    validation_actions = []\n",
    "\n",
    "                    for k in range(1, X_val.shape[0]):\n",
    "                        val_action = value_network(val_state).argmax()\n",
    "\n",
    "                        val_next_state = X_val[k]\n",
    "                        val_next_state[..., -1] = val_action\n",
    "\n",
    "                        val_next_bid = bid_val[k]\n",
    "                        val_next_ask = ask_val[k]\n",
    "\n",
    "                        reward = get_reward(val_state, val_bid, val_ask, val_next_state, val_next_bid, val_next_ask, neutral_penalty=0)\n",
    "                        validation_reward += reward\n",
    "                        validation_actions.append(val_action.item())\n",
    "                        val_state = val_next_state\n",
    "                        val_bid = val_next_bid\n",
    "                        val_ask = val_next_ask\n",
    "\n",
    "                    mlflow.log_metric(\"validation_episode_reward\", validation_reward, step=epoch)\n",
    "                    for key, value in dict(pd.Series(validation_actions).value_counts()).items():\n",
    "                        mlflow.log_metric(f\"validation_action_{key}\", value, step=epoch)\n",
    "\n",
    "                    if validation_reward > best_episode_validation_reward:\n",
    "                        best_episode_validation_reward = validation_reward\n",
    "                        torch.save(value_network.state_dict(), \"best_val_value_state_dict.pt\")\n",
    "                        mlflow.log_artifact(\"best_val_value_state_dict.pt\")\n",
    "                    value_network.train()\n",
    "\n",
    "            torch.save(value_network.state_dict(), \"last_train_value_state_dict.pt\")\n",
    "            mlflow.log_artifact(\"last_train_value_state_dict.pt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e44b0-d3ed-4521-9fa3-61cb55d775a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DQN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
