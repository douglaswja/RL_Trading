{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bebc7624-238b-45ea-b7f9-d7daa0f007f7",
   "metadata": {},
   "source": [
    "# Chapter 3 - Finite Markov Decision Processes\n",
    "**Reinforcement Learning: An Introduction, by Richard S. Sutton and Andrew G. Barto**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3da1bfe-112b-427c-808c-172f1b256cda",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87174fc3-8f42-4c2d-a0c3-eb6b053a2ce7",
   "metadata": {},
   "source": [
    "- An agent's attempt to maximize reward should make it achieve our goals. We should not impart prior knowledge to the agent through rewards, i.e. reward for actually winning and not for achieving subgoals because the agent might find a way to achieve these subgoals without achieving the real goal. **Rewards communicate what we want achieved and not how we want them achieved**\n",
    "- Tasks can be episodic (have a terminal state) or continuous (go on forever). Continuous tasks use the discounting factor $\\gamma$ to make them mathematically feasible while episodic tasks must define terminal state(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e4d2b1-bcb1-4ab1-b26a-5b2cadca4237",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58283c39-e99f-49b1-a43e-f65ea0601473",
   "metadata": {},
   "source": [
    "### 3.1 Examples of MDP framework\n",
    "\n",
    "1. Financial trading:\n",
    "- State: Past 3 months of data from the market\n",
    "- Actions: How/when to execute new trades and manage open trades\n",
    "- Rewards: Financial gain\n",
    "\n",
    "2. Playing league of legends:\n",
    "- State: Current screen frame and 5 screen frames prior, including information on items each champion has, where they were last seen and at what time, the spawn timers of jungle camps and minions, the known cooldowns of each champion, the known buffs each champion has, and any death timers.\n",
    "- Actions: Mouse action and ability activation that each champion on our team should take, including shopping for items.\n",
    "- Reward: +1000 if we win the game.\n",
    "\n",
    "3. Managing container ships:\n",
    "- State: The location, velocity, cargo status, destination, and geographical / legal requirements that each ship is currently experiencing.\n",
    "- Actions: Velocity instructions to each ship\n",
    "- Reward: Positive reward for delivering goods safely and inaccordance of all requirements, negative reward for each timestep that passes to compel the reinforcement learning agent to deliver goods efficiently. Negativie reward for using too much resources too, like fuel from going too fast or food from the crew being delayed at sea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee8dcf-a7df-4b57-a519-a8ce5619095d",
   "metadata": {},
   "source": [
    "### 3.2 Adequacy of MDP framework\n",
    "\n",
    "The framework fails if we need to optimise for more than 1 reward value (e.g. a tuple of rewards) that cannot be expressed as a single number.\n",
    "\n",
    "For example, while economic concerns can be expressed as rewards in terms of monetary amount, if we want to balance economic concerns with religious ones then these cannot be feasibly expressed together as a single number.\n",
    "\n",
    "One could try to choose only one of the rewards to optimise for under different states. However, the eventual overall reward will have two separate metrics which cannot be combined and hence we cannot tell if it is better than some other balance of the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b86248-4e10-49bc-9c66-0c0435487810",
   "metadata": {},
   "source": [
    "### 3.3 Where to draw the line\n",
    "\n",
    "Draw the line at the level of abstraction of interest to the designer of the algorithm. There is a certain level at which we are interested in studying or that we can act upon to enact changes. For example, a business would be interested in maximising its profits through all the facets it has available to it and anything beyond its control would be of secondary concern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f6d24d-9d66-4b43-a2ea-e18df4accffe",
   "metadata": {},
   "source": [
    "### 3.4 Table analogous to example 3.3\n",
    "\n",
    "|s|a|s'|r|p(s',r\\|s,a)|\n",
    "|---|---|---|---|---|\n",
    "|high|search|high|$r_{search}$|$\\alpha$|\n",
    "|high|search|low|$r_{search}$|$1-\\alpha$|\n",
    "|high|wait|high| $r_{wait}$ |1|\n",
    "|low|search|low| $r_{search}$ | $\\beta$ |\n",
    "|low|search|high| $-3$ | $1 - \\beta$ |\n",
    "|low|wait|low|$r_{wait}$|1|\n",
    "|low|recharge|high|0|1|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608891e3-a6dd-45f5-a085-d27eaaa01e12",
   "metadata": {},
   "source": [
    "### 3.5 Modifications to apply to episodic\n",
    "\n",
    "Continuing vs episodic:\n",
    "\n",
    "- Continuing tasks go on continually without limit\n",
    "- Episodic tasks have a final time step\n",
    "\n",
    "Equation (3.3) would be modified to include the terminal state, i.e. use $S^+$ rather than $S$.\n",
    "The probability of leaving a terminal state should also always be zero for all actions.\n",
    "\n",
    "Or to talk about episode number, i.e. whether this is the first run of the whole experiment or the second run of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb04d15-c845-4cc0-8a6c-bcc7bcaf4cec",
   "metadata": {},
   "source": [
    "### 3.6\n",
    "It would be the same, no difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74541d-8492-427f-9f4b-81d4a9d82e21",
   "metadata": {},
   "source": [
    "### 3.7\n",
    "\n",
    "The agent is not being penalized for how long it takes to escape the maze, so as long as it eventually does so it is achieving the goals set out for it.\n",
    "\n",
    "To show \"improvement in escaping from the maze\" we want the robot to escape more quickly, and should hence penalise the agent according to how long it takes to exit the maze by penalizing it -1 for each timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc15bd5-34f3-4e85-96e6-5c0208e74084",
   "metadata": {},
   "source": [
    "### 3.8\n",
    "\n",
    "$G_5 = 0$  \n",
    "$G_4 = 2$  \n",
    "$G_3 = 4$  \n",
    "$G_2 = 8$  \n",
    "$G_1 = 6$  \n",
    "$G_0 = 2$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29110b6-beb3-4355-a4c6-eb53e8ff2a73",
   "metadata": {},
   "source": [
    "### 3.9\n",
    "$G_1 = 70$  \n",
    "$G_0 = 65$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03c15f9-8251-4693-84a2-968398b9b400",
   "metadata": {},
   "source": [
    "### 3.10\n",
    "Sum of geometric progression.  \n",
    "\n",
    "First term is 1, multiplicative factor is $\\gamma$, the formula is: $\\frac{a}{1-r} = \\frac{1}{1-\\gamma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded8099-8f5c-4c68-aaeb-5055b801986d",
   "metadata": {},
   "source": [
    "### 3.11\n",
    "$E[R_{t+1}] = \\sum_{a \\in A} \\pi(a | s) \\left( \\sum_{r \\in R} r \\left( \\sum_{s' \\in S} P(s', r | s, a) \\right) \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55489885-349a-4c4d-8f4c-50162ec4b663",
   "metadata": {},
   "source": [
    "### 3.12\n",
    "$v_{\\pi}(s) = \\sum_{a \\in A} q_{\\pi}(s, a) \\pi(a|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d06600-3794-4456-97fd-c604a14e45d9",
   "metadata": {},
   "source": [
    "### 3.13\n",
    "$q_{\\pi}(s, a) = \\sum_{s' \\in S} v_{\\pi}(s') \\left( \\sum_{r \\in R} P(s', r | s, a) \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d290f57-5539-4203-80bd-2a884a3e5922",
   "metadata": {},
   "source": [
    "### 3.15\n",
    "$v_c = \\frac{c}{1-\\gamma}$\n",
    "\n",
    "$v_\\pi(s_t) = E_\\pi[G_t | S = s_t] + v_c$\n",
    "\n",
    "$\\therefore v_\\pi(s_A) - v_\\pi(s_B) == v_\\pi(s_{A|c}) - v_\\pi(s_{B|c})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54294921-9767-45a3-8564-832f349ed3d6",
   "metadata": {},
   "source": [
    "### 3.16\n",
    "Would adding c to an episodic task have an effect?\n",
    "\n",
    "I think that it would because we only want the agent to maximize the reward according to our goals. Allowing rewards for wasting time will make the agent want to take longer to complete the maze. In this case sign matters.\n",
    "\n",
    "In conclusion, the relative difference between states doesnt change if we add a constant reward C to all states. But this can change the behaviour of the agent because time-based penalties could become rewards instead, encouraging stalling behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e88e57e-5383-4ecf-bbc0-16afbd46fcb5",
   "metadata": {},
   "source": [
    "### 3.17\n",
    "$q_\\pi(s,a) = r + \\sum_{s' \\in S} \\frac{P(s', r | s, a)}{\\sum_{s'' \\in S} P(s'', r | s, a)} \\left( \\sum_{a \\in A} \\pi(a | s') q(s'|a) \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9014c5d-a2c9-4bc1-86bb-262e2201f79b",
   "metadata": {},
   "source": [
    "### 3.18\n",
    "\n",
    "$v_\\pi(s_{t}) = E_\\pi[q_\\pi(s_{t}, a) | S_t = s_{t}] = \\sum_{a \\in A} \\pi(a | s) q_\\pi(s, a)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
