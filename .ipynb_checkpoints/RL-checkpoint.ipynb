{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c1de9e-b45c-4e1a-81c2-9d403778b012",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CHECK ACTION INPUT OUTPUT INTO MODELS (DO WE NEED TO -1)\n",
    "# CHECK REWARD FUNCTION\n",
    "# CHECK SAMPLE_SIZE=1 DATA\n",
    "# CHECK STANDARDIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68543e12-9c91-4d1f-8aab-2e1e596e8d83",
   "metadata": {
    "tags": []
   },
   "source": [
    "0. Pretraining\n",
    "1. Toggle midprice\n",
    "2. Toggle features [Could use an encoding layer to lower the state space down]\n",
    "3. 3 algorithms\n",
    "    - Might need to clip gradients and normalize rewards, anything to maintain stability.\n",
    "    - Include the price the asset was bought at in the state\n",
    "    - Jumble things (with randomization) [Optional with MASSIVE [1million] replay memory]\n",
    "    - Train-validation-test splits with continuous training during execution (During training we'll use the jumble)\n",
    "4. Adjust rewards (e.g. multiply negative penalties by 2 to make it twice as bad to lose money as to make money)\n",
    "\n",
    "Considerations:\n",
    "- Proximal Policy Optimization (PPO)\n",
    "- Soft Actor Critic (SAC)\n",
    "- Encoder layer for states\n",
    "\n",
    "Next:\n",
    "- Could also change the sample size from 20 and vary it down to 1, consider some form of RNN if possible (idk how to do that with batch training though)\n",
    "- Need to figure out MLflow logging\n",
    "- Tensorboard: https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html\n",
    "- Long-only policy\n",
    "- Recreating the table on Slide 41 of 56 from this set of lecture slides would be good for our purposes https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf\n",
    "\n",
    "\n",
    "Datasets (Standardize all):\n",
    "- Minimal\n",
    "- Minimal window\n",
    "- Full\n",
    "- Full window\n",
    "- Encoder of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03432b8f-6499-49d9-aece-158eaf854b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import mlflow\n",
    "from collections import namedtuple\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "from LOB_analysis import *\n",
    "from commons.replay_memory import ReplayMemory, Experience\n",
    "\n",
    "%matplotlib inline\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1cb7aa6-55e3-47f2-bd67-5b8e6831f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKET_OPEN = '9:30'\n",
    "MARKET_CLOSE = '16:00'\n",
    "GRANULARITY = '15 s'\n",
    "MONTH = 'jan'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44276675-d696-47b0-b9a3-8ae577dde10e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed0e6134-47de-4b29-b506-e668e81c54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "dataset_scopes = ['minimal', 'minimal_window', 'full', 'full_window']\n",
    "dataset_components = [\n",
    "    'X_train.pt', 'X_validation.pt', 'X_test.pt',\n",
    "    'y_train.pt', 'y_validation.pt', 'y_test.pt',\n",
    "    'bid_train.pt', 'bid_val.pt', 'bid_test.pt',\n",
    "    'ask_train.pt', 'ask_val.pt', 'ask_test.pt',\n",
    "]\n",
    "\n",
    "for dataset_scope in dataset_scopes:\n",
    "    dataset = []\n",
    "    for dataset_component in dataset_components:\n",
    "        dataset.append(\"datasets/\" + MONTH + \"/\" + dataset_scope + \"/\" + dataset_component)\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e71aa7a-5a5b-424b-af7c-d8a8299841ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/jan/minimal/X_train.pt',\n",
       " 'datasets/jan/minimal/X_validation.pt',\n",
       " 'datasets/jan/minimal/X_test.pt',\n",
       " 'datasets/jan/minimal/y_train.pt',\n",
       " 'datasets/jan/minimal/y_validation.pt',\n",
       " 'datasets/jan/minimal/y_test.pt',\n",
       " 'datasets/jan/minimal/bid_train.pt',\n",
       " 'datasets/jan/minimal/bid_val.pt',\n",
       " 'datasets/jan/minimal/bid_test.pt',\n",
       " 'datasets/jan/minimal/ask_train.pt',\n",
       " 'datasets/jan/minimal/ask_val.pt',\n",
       " 'datasets/jan/minimal/ask_test.pt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf160cb0-9cf7-4108-b159-28813b07d299",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature engineering\n",
    "\n",
    "**I do not know if standardization is actually a good thing. [CHECK - Ask prof too]**  \n",
    "Standardization will be done using a small window of previous events. This is because the statistical properties change as time moves on, so we standardize w.r.t small prior windows. Standardization benefits ML training since values will stradle 0 and not be too far from it.  \n",
    "\n",
    "**Check if we should reindex time-series data with forward padding and explicitly 0-labels given to these regions.**\n",
    "\n",
    "**BE CAREFUL OF THE CURSE OF DIMENSIONALITY**\n",
    "To understand if the curse of dimensionality is affecting performance, rerun the exact same experiments with fewer features and see if profitability increases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa823b85-caf0-4604-80f7-1c2147a354c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = pd.read_pickle(f\"../data/AMD_{MONTH}_msg.pickle\")\n",
    "orderbook = pd.read_pickle(f\"../data/AMD_{MONTH}_odb.pickle\")\n",
    "\n",
    "market_open_idx = message.set_index('time').index.indexer_between_time(MARKET_OPEN, MARKET_CLOSE)\n",
    "\n",
    "message = message.iloc[market_open_idx, :].reset_index(drop=True)\n",
    "orderbook = orderbook.iloc[market_open_idx, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a2beb-e7b5-4718-94be-2282020c14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_data(message, orderbook, granularity=GRANULARITY, standardize_offset = 1) # Standardize by data of 1 day prior\n",
    "\n",
    "bids = data.loc[:, 'bid']\n",
    "asks = data.loc[:, 'ask']\n",
    "data = data.drop(['bid', 'ask'], axis=1)\n",
    "# data.to_pickle(\"datasets/jan/full/dataframe.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1fb670-8efb-4ebc-9e10-4dd62ad0184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For optimizing the threshold value to produce balanced class labels.\n",
    "\n",
    "Use with scipy.optimize.minimize_scalar\n",
    "\"\"\"\n",
    "def label_eval(threshold, time_horizon = 50):\n",
    "    labels = get_label(data.mid_price, time_horizon = time_horizon, threshold = threshold)\n",
    "    counts = labels.value_counts()\n",
    "    return -(counts / counts.sum()).min()\n",
    "\n",
    "threshold = minimize_scalar(label_eval, bounds=(0, 2)).x\n",
    "labels = get_label(data.mid_price, time_horizon=50, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b0f75-49fc-4d95-8c8a-d44c78ed8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913cd36b-cb1d-4365-9e4c-1853470f3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, bids, asks = generate_time_series_samples(data, labels, bids, asks, SAMPLE_SIZE) # This fn performs dropna\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd010e8d-9554-4155-ac6a-72de45e9e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (y.index == asks.index).all()\n",
    "assert (y.index == bids.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb250f-df69-461d-8e39-86b263511c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flatten = X.reshape(X.shape[0], -1)\n",
    "X_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8695be0-f655-4aae-b1e8-4f911785ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch = torch.ones((X_flatten.shape[0], X_flatten.shape[1] + 1))\n",
    "X_torch[:, :-1] = torch.tensor(X_flatten)\n",
    "X_torch = X_torch.to(device)\n",
    "X_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65a05e-7e1a-45f2-b85e-f53cda6e0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape, bids.shape, asks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd6251-3acf-4093-98f5-4dca317e9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6442491-f162-41a0-a129-2b7a3c8f6ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor = torch.FloatTensor(y + 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a691bd6-2116-442e-8cd1-0098f6d86a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tv, X_test, y_tv, y_test, bid_tv, bid_test, ask_tv, ask_test = train_test_split(X_torch, y_tensor, bids, asks, shuffle=False, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val, bid_train, bid_val, ask_train, ask_val = train_test_split(X_tv, y_tv, bid_tv, ask_tv, shuffle=False, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0286b72-1fcb-428d-a37d-3fc264c32315",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datasets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e5e10-fea6-4faf-aecb-41847f66f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate([X_train, X_val, X_test, y_train, y_val, y_test, bid_train, bid_val, bid_test, ask_train, ask_val, ask_test]):\n",
    "    torch.save(data, dt[i])\n",
    "    print(type(data), data.shape, dt[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940bcf6-ce29-4320-8d92-ed2956603ab2",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Note that actions are:\n",
    "- 0: Short   (Have -1 asset)\n",
    "- 1: Neutral (Have 0 asset)\n",
    "- 2: Long    (Have 1 asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b0bcf8-47ba-469e-a674-747b5519c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\"\"\"Action-Value Function := Q(s, a)\"\"\"\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_dim),\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear_stack(x)\n",
    "\n",
    "\"\"\"Value Function := V(s)\"\"\"\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear_stack(x)\n",
    "\n",
    "\"\"\"Policy Function := P(a | s)\"\"\"\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_dim),\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x, return_logits=False):\n",
    "        logits = self.linear_stack(x)\n",
    "        if return_logits:\n",
    "            return logits\n",
    "        \n",
    "        probs = Categorical(logits=logits)\n",
    "        action = probs.sample()\n",
    "        return action\n",
    "    \n",
    "    def get_log_prob(self, state, action):\n",
    "        logits = self.linear_stack(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        return dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb880ebe-f412-4a5f-9dae-855ba07b62d0",
   "metadata": {},
   "source": [
    "## Reward function\n",
    "\n",
    "State representation:\n",
    "\n",
    "- At any current point we want to know our current portfolio\n",
    "- The action is how we want to the new portfolio that we want, we will limit our portfolio (and hence actions) to -1, 0, and 1 quantity of the asset  \n",
    "- We look at the next time step and get reward from the action we took\n",
    "\n",
    "This neutral penalty seems to work: $0.0001 * 10^{epoch}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6a0cef-00e7-49f1-95bc-6afd27edbf3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_reward(prev_state, prev_bid, prev_ask, curr_state, curr_bid, curr_ask, use_midprice=False, prev_action_idx=-1, trading_fee=1e-4, neutral_penalty=1e-6):\n",
    "    valid_actions = [0, 1, 2]\n",
    "    assert (prev_state.dim() == 1) and (curr_state.dim() == 1), \"State provided is not 1 dimensional\"\n",
    "    assert (prev_state[prev_action_idx] in valid_actions) and (curr_state[prev_action_idx] in valid_actions), \"Action provided is not in [0, 1, 2] range\"\n",
    "    \n",
    "    prev_is_neutral = prev_state[prev_action_idx] == 1\n",
    "    curr_is_neutral = curr_state[prev_action_idx] == 1\n",
    "    \n",
    "    spread = 0 if use_midprice else (prev_ask - prev_bid)\n",
    "    \n",
    "    if curr_is_neutral:\n",
    "        if prev_is_neutral:\n",
    "            # Neutral -> Neutral\n",
    "            return -neutral_penalty\n",
    "\n",
    "        if not prev_is_neutral:\n",
    "            # Active -> Neutral\n",
    "            return -(trading_fee)\n",
    "    \n",
    "    # Here onwards current portfolio is active\n",
    "    curr_is_long = curr_state[prev_action_idx] == 2 # True -> Long, False -> Short\n",
    "    if use_midprice and curr_is_long:\n",
    "        price_change = ((curr_ask + curr_bid) / 2) - ((prev_ask + prev_bid) / 2)\n",
    "    elif use_midprice and not curr_is_long:\n",
    "        price_change = ((prev_ask + prev_bid) / 2) - ((curr_ask + curr_bid) / 2)\n",
    "    elif not use_midprice and curr_is_long:\n",
    "        price_change = curr_ask - prev_ask\n",
    "    else:\n",
    "        price_change = prev_bid - curr_bid\n",
    "        \n",
    "    portfolio_is_same = prev_state[prev_action_idx] == curr_state[prev_action_idx]\n",
    "    \n",
    "    if prev_is_neutral:\n",
    "        # Neutral -> Active\n",
    "        return -trading_fee + price_change\n",
    "    \n",
    "    if not prev_is_neutral:\n",
    "        if portfolio_is_same:\n",
    "            # Active -> Active\n",
    "            return price_change\n",
    "        else:\n",
    "            # Active -> -Active\n",
    "            return -(spread + trading_fee) - trading_fee + price_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17da243-b7cb-4e0e-b9aa-36df24d98386",
   "metadata": {},
   "source": [
    "## 1. Advantage Actor Critic (A2C) RL\n",
    "\n",
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a82735c-3427-4ec5-8ab1-176eebb4247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SPACE = [0, 1, 2]\n",
    "\n",
    "OUTPUT_DIMS = 3\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "N_PRETRAIN_EPOCH = 150\n",
    "\n",
    "N_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "REPLAY_MEMORY_SIZE = 1048576\n",
    "DISCOUNT_RATE = 0.99       # Discounted rate of future returns\n",
    "FROZEN_UPDATE_INTERVAL = 16\n",
    "\n",
    "TRADING_FEE = 1e-4\n",
    "NEUTRAL_PENALTY = 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f30a84-faf4-4a99-a1d8-7f430fe176e3",
   "metadata": {},
   "source": [
    "### Exploration-exploitation parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1de2552-f20e-4b0f-86cf-69d5bbb86311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_exploration_rate = 0.001\n",
    "max_exploration_rate = 1.\n",
    "exploration_decay_rate = 0.05\n",
    "ers = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*epoch)\n",
    "    ers.append(exploration_rate)\n",
    "pd.Series(ers).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2bff5-8406-4e66-830d-8c8451d614a6",
   "metadata": {},
   "source": [
    "### Pretrain Actor Network then commence RL training\n",
    "\n",
    "Use the labels computed above to pretrain the actor network.  \n",
    "All portfolio values [0, 1, 2] are trained to output logits that best suit the given labels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88a240-a3db-4d6b-9686-bfceea21a770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149 Batch:  25 of 29 Loss:   1.299151\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"A2C\")\n",
    "for dataset in datasets:\n",
    "    for USE_MIDPRICE in [True, False]:\n",
    "        # Fetch data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, bid_train, bid_val, bid_test, ask_train, ask_val, ask_test = [torch.load(path) for path in dataset]\n",
    "        y_train, y_val, y_test = y_train.long(), y_val.long(), y_test.long()\n",
    "        INPUT_DIMS = X_train.shape[-1]\n",
    "\n",
    "        policy = ActorNetwork(input_dim=INPUT_DIMS, output_dim=OUTPUT_DIMS).to(device)\n",
    "        critic = CriticNetwork(input_dim=INPUT_DIMS).to(device)\n",
    "        critic_frozen = CriticNetwork(input_dim=INPUT_DIMS).to(device)\n",
    "        critic_frozen.load_state_dict(critic.state_dict())\n",
    "        \n",
    "                \n",
    "        # Begin logging\n",
    "        with mlflow.start_run() as run:\n",
    "            mlflow.log_params({\n",
    "                'RL_algorithm': \"A2C\",\n",
    "                'batch_size': BATCH_SIZE,\n",
    "                'n_pretrain_epoch': N_PRETRAIN_EPOCH,\n",
    "                'n_epochs': N_EPOCHS,\n",
    "                'learning_rate': LEARNING_RATE,\n",
    "                'replay_memory_size': REPLAY_MEMORY_SIZE,\n",
    "                'discount_rate': DISCOUNT_RATE,\n",
    "                'frozen_update_interval': FROZEN_UPDATE_INTERVAL,\n",
    "                'use_midprice': USE_MIDPRICE,\n",
    "                'trading_fee': TRADING_FEE,\n",
    "                'neutral_penalty': NEUTRAL_PENALTY,\n",
    "                'market_open': MARKET_OPEN,\n",
    "                'market_close': MARKET_CLOSE,\n",
    "                'granularity': GRANULARITY,\n",
    "                'month': MONTH,\n",
    "                'datascope': dataset[0].split(\"/\")[2],\n",
    "                'device': device,\n",
    "                'input_dims': INPUT_DIMS,\n",
    "                'critic_loss_fn': 'MSE',\n",
    "                'network_optimizer_fns': 'Adam',\n",
    "                'pretrain_loss_fn': 'CrossEntropyLoss',\n",
    "                'policy_network': str(policy.linear_stack).replace(\"in_features=\", \"\").replace(\"out_features=\", \"\").replace(\" \", \"\").replace(\"\\n\", \"\").replace(\",bias=True\", \"\"),\n",
    "                'critic_network': str(critic.linear_stack).replace(\"in_features=\", \"\").replace(\"out_features=\", \"\").replace(\" \", \"\").replace(\"\\n\", \"\").replace(\",bias=True\", \"\"),\n",
    "                'min_exploration_rate': min_exploration_rate,\n",
    "                'max_exploration_rate': max_exploration_rate,\n",
    "                'exploration_decay_rate': exploration_decay_rate,\n",
    "            })\n",
    "\n",
    "            policy.train()\n",
    "            critic.train()\n",
    "            critic_frozen.eval()\n",
    "\n",
    "            policy_optimizer = optim.Adam(params=policy.parameters(), lr=LEARNING_RATE)\n",
    "            critic_optimizer = optim.Adam(params=critic.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "            mse_fn = nn.MSELoss()\n",
    "\n",
    "            dataloader = DataLoader([(X_train[i], y_train[i]) for i in range(X_train.shape[0])], shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "            pretrain_loss_fn = nn.CrossEntropyLoss()\n",
    "            pretrain_optimizer = optim.Adam(params=policy.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "            for i in range(N_PRETRAIN_EPOCH):\n",
    "                for batch, (X, y) in enumerate(dataloader):\n",
    "                    X_dummy = X.clone()\n",
    "                    X_dummy[..., -1] = i % 3\n",
    "\n",
    "                    logits = policy(X_dummy, return_logits=True)\n",
    "                    pretrain_loss = pretrain_loss_fn(logits, y.long())\n",
    "\n",
    "                    # Backpropagation\n",
    "                    pretrain_optimizer.zero_grad()\n",
    "                    pretrain_loss.backward()\n",
    "                    pretrain_optimizer.step()\n",
    "\n",
    "                    if batch % 5 == 0:\n",
    "                        print(f\"Epoch: {i:2} Batch: {batch:3} of {len(dataloader)} Loss: {pretrain_loss.item():10.6f}\", end=\"\\r\")\n",
    "                mlflow.log_metric(\"pretrain_loss\", pretrain_loss.item(), step=i)\n",
    "\n",
    "            print(\"\")\n",
    "\n",
    "            critic_loss_history = []\n",
    "            policy_loss_history = []\n",
    "            reward_history = []\n",
    "            replay_memory = ReplayMemory(REPLAY_MEMORY_SIZE) # 2**20 values\n",
    "            \n",
    "            best_episode_reward = float(\"-inf\")\n",
    "\n",
    "            for epoch in range(N_EPOCHS):\n",
    "                done = False\n",
    "                episode_reward = 0\n",
    "                state = X_train[0].clone()\n",
    "                bid, ask = bid_train[0], ask_train[0]\n",
    "\n",
    "                for i in range(1, X_train.shape[0]):\n",
    "                    # Explore vs exploit\n",
    "                    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*epoch)\n",
    "                    if random.uniform(0, 1) > exploration_rate:\n",
    "                        action = policy(state) # Exploit\n",
    "                    else:\n",
    "                        action = torch.tensor(random.sample(ACTION_SPACE, 1)).to(device) # Explore\n",
    "\n",
    "                    # Environment step\n",
    "                    next_state = X_train[i].clone()\n",
    "                    next_state[..., -1] = action\n",
    "                    next_bid = bid_train[i]\n",
    "                    next_ask = ask_train[i]\n",
    "\n",
    "                    reward = get_reward(state, bid, ask, next_state, next_bid, next_ask, use_midprice=USE_MIDPRICE, trading_fee=TRADING_FEE, neutral_penalty=NEUTRAL_PENALTY)\n",
    "                    done = i == (X_train.shape[0] - 1)\n",
    "\n",
    "                    # Saves\n",
    "                    replay_memory.push(Experience(state, action, reward, next_state, done))\n",
    "                    episode_reward += reward\n",
    "                    state = next_state\n",
    "                    bid = next_bid\n",
    "                    ask = next_ask\n",
    "\n",
    "                    # High frequency logs\n",
    "                    mlflow.log_metric(\"intermediate_actions\", action.item()) # LOG THE ACTIONS THAT THE AGENT IS TAKING, really need to see if portfolio is changing or not\n",
    "                    mlflow.log_metric(\"intermediate_rewards\", reward)\n",
    "\n",
    "                    if replay_memory.can_provide_sample(BATCH_SIZE):\n",
    "                        states, actions, rewards, next_states, dones = replay_memory.sample(BATCH_SIZE)\n",
    "                        states, actions, rewards, next_states, dones = states.to(device), actions.to(device), rewards.to(device), next_states.to(device), dones.to(device)\n",
    "                        actions = actions.long()\n",
    "                        dones = dones.float()\n",
    "\n",
    "                        # Error calculation\n",
    "                        log_probs = policy.get_log_prob(states, actions)\n",
    "                        q_values = critic(states).squeeze(-1)\n",
    "                        q_target = (rewards + (1 - dones) * DISCOUNT_RATE * critic_frozen(next_states).squeeze(-1)).detach()\n",
    "                        advantages = q_target - q_values\n",
    "\n",
    "                        assert (log_probs <= 0).all(), \"There cannot be positive log probabilities\"\n",
    "\n",
    "                        policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "                        value_loss = mse_fn(q_target, q_values)\n",
    "\n",
    "                        # Optimization\n",
    "                        critic_optimizer.zero_grad()\n",
    "                        value_loss.backward()\n",
    "                        critic_optimizer.step()\n",
    "\n",
    "                        policy_optimizer.zero_grad()\n",
    "                        policy_loss.backward()\n",
    "                        policy_optimizer.step()\n",
    "\n",
    "                reward_history.append(episode_reward) ######## LOG THE REWARD\n",
    "                mlflow.log_metric(\"episode_reward\", episode_reward, step=epoch)\n",
    "\n",
    "                if replay_memory.can_provide_sample(BATCH_SIZE) and i % FROZEN_UPDATE_INTERVAL == 0:\n",
    "                    critic_frozen.load_state_dict(critic.state_dict())\n",
    "\n",
    "                if replay_memory.can_provide_sample(BATCH_SIZE):\n",
    "                        print(f\"Epoch: {epoch:>5,d}, Reward: {episode_reward:>10,.6f}, Critic loss: {value_loss.item():>10,.6f}, Actor loss: {policy_loss.item():>10,.6f}\", end=\"\\r\")\n",
    "                        mlflow.log_metric(\"critic_loss\", value_loss.item(), step=epoch)\n",
    "                        mlflow.log_metric(\"actor_loss\", policy_loss.item(), step=epoch)\n",
    "                \n",
    "                if (episode_reward > best_episode_reward):\n",
    "                    best_episode_reward = episode_reward\n",
    "                    torch.save(policy.state_dict(), \"best_policy_state_dict.pt\")\n",
    "                    torch.save(critic.state_dict(), \"best_critic_state_dict.pt\")\n",
    "                    mlflow.log_artifact(\"best_policy_state_dict.pt\")\n",
    "                    mlflow.log_artifact(\"best_critic_state_dict.pt\")\n",
    "            torch.save(policy.state_dict(), \"last_policy_state_dict.pt\")\n",
    "            torch.save(critic.state_dict(), \"last_critic_state_dict.pt\")\n",
    "            mlflow.log_artifact(\"last_policy_state_dict.pt\")\n",
    "            mlflow.log_artifact(\"last_critic_state_dict.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098e5a3-a6d7-49bd-a33f-cc295607a686",
   "metadata": {},
   "source": [
    "Epoch:     0, Reward:  -1.845400, Critic loss:   0.024456, Actor loss: -301.728149"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1614b974-a928-4d93-9e6c-11ab7fcc4839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a2c_reward_history = pd.Series(reward_history)\n",
    "a2c_reward_history.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ed8cf-74e6-45f0-b269-8d55b05979ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_hist = pd.Series(portfolio_history)\n",
    "(portfolio_hist != portfolio_hist.shift(1).fillna(portfolio_hist[0])).sum()\n",
    "# The number of trades executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d144c64-86ac-41af-9698-7ab1e974d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_hist.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957b22a-21c7-43d8-9cb1-29f6dff29b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reward_history[:]).cumsum().plot(marker='.')\n",
    "plt.ylabel(\"Cumulative rewards\")\n",
    "plt.xlabel(\"Time step\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcdb7c4-bdc9-4553-be16-60f84ef105b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_hist = pd.Series(portfolio_history)\n",
    "(portfolio_hist != portfolio_hist.shift(1).fillna(portfolio_hist[0])).sum()\n",
    "# The number of trades executed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b67e6c8-5a1c-43d3-9791-a13a199da070",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_hist.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6419567b-47f1-40bc-bdc9-66dc902040ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reward_history).cumsum().plot(marker='.')\n",
    "plt.ylabel(\"Cumulative rewards\")\n",
    "plt.xlabel(\"Time step\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77902a7a-252e-437a-ad87-ce54f2775c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reward_history[:2000]).cumsum().plot()\n",
    "plt.ylabel(\"Cumulative rewards\")\n",
    "plt.xlabel(\"Time step\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c754ad-6e40-48bd-86c4-bea032ea3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(reward_history[20000:]).cumsum().plot()\n",
    "plt.ylabel(\"Cumulative rewards\")\n",
    "plt.xlabel(\"Time step\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b97689-d3a1-4387-92d9-8eda94102779",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Optional parameter save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ff124-ecee-4baf-bd34-9b2fbca0ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), r\"../logs/policy_model_increasing_neutral_cost.pt\")\n",
    "torch.save(critic.state_dict(), r\"../logs/critic_model_increasing_neutral_cost.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4122ddc-949e-4d97-9c8f-e3f6b5d63655",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DQN (ACTUALLY, DO AC, DDQN, then DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af53f4-938e-4481-b203-9be96951d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over datasets\n",
    "# Iterate over midprice and reward function\n",
    "# Iterate over hyperparameters\n",
    "# RL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b60964-a28f-4d27-b02c-237237b2f148",
   "metadata": {},
   "source": [
    "Final report\n",
    "- Describe RL\n",
    "- Describe LOB\n",
    "- What metric are we trying to optimise\n",
    "- Inputs\n",
    "- Literature review\n",
    "- Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
