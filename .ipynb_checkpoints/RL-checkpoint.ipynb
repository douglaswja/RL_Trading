{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "956fac81-a2f1-4298-b220-863df96389e0",
   "metadata": {},
   "source": [
    "Enhancements:\n",
    "- Include price that assets were bought into the state\n",
    "- Explore neutral penalty\n",
    "- Encoding layer to control/learn state representation\n",
    "- Improve stability of learning algorithms through the use of PPO, SAC\n",
    "- Create short-term endpoints for trading windows during training and shuffle these windows\n",
    "- Feature selection and hyperparameter tuning through validation search [Discount rate, ]\n",
    "- More sophistication for state-propagation, e.g. Recurrent NN, Transformer\n",
    "\n",
    "TODO:\n",
    "- Walk-forward validation training\n",
    "- Test evaluation of all models\n",
    "- Use trained value network and pretrained policy network?\n",
    "\n",
    "\n",
    "Consider:\n",
    "- Recreating the table on Slide 41 of 56 from this set of lecture slides would be good for our purposes https://www.davidsilver.uk/wp-content/uploads/2020/03/FA.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8449d979-6698-419c-9ffc-8c9a7fca78d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Experience = namedtuple(\"Experience\", ['state', 'action', 'reward', 'next_state', 'done'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8fd41b9-4319-43dc-9fb0-7d4cf7fe2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity: int):\n",
    "        self._capacity = capacity\n",
    "        self._memory = []\n",
    "        self._count = 0\n",
    "    \n",
    "    def push(self, experience: Experience) -> None:\n",
    "        if len(self._memory) < self._capacity:\n",
    "            self._memory.append(experience)\n",
    "        else:\n",
    "            self._memory[self._count % self._capacity] = experience\n",
    "        self._count += 1\n",
    "    \n",
    "    def sample(self, batch_size: int) -> Experience:\n",
    "        batch = Experience(*zip(*random.sample(self._memory, batch_size)))\n",
    "        t1 = torch.stack(batch.state).float()\n",
    "        t2 = torch.tensor(batch.action).float()\n",
    "        t3 = torch.tensor(batch.reward).float()\n",
    "        t4 = torch.stack(batch.next_state).float()\n",
    "        t5 = torch.tensor(batch.done).float()\n",
    "        return Experience(state=t1, action=t2, reward=t3, next_state=t4, done=t5)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size: int) -> bool:\n",
    "        return len(self._memory) >= batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03432b8f-6499-49d9-aece-158eaf854b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import mlflow\n",
    "from collections import namedtuple\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "from LOB_analysis import *\n",
    "\n",
    "%matplotlib inline\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1cb7aa6-55e3-47f2-bd67-5b8e6831f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKET_OPEN = '9:30'\n",
    "MARKET_CLOSE = '16:00'\n",
    "GRANULARITY = '15 s'\n",
    "MONTH = 'jan'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44276675-d696-47b0-b9a3-8ae577dde10e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0e6134-47de-4b29-b506-e668e81c54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "dataset_scopes = ['minimal', 'minimal_window', 'full', 'full_window']\n",
    "dataset_components = [\n",
    "    'X_train.pt', 'X_validation.pt', 'X_test.pt',\n",
    "    'y_train.pt', 'y_validation.pt', 'y_test.pt',\n",
    "    'bid_train.pt', 'bid_val.pt', 'bid_test.pt',\n",
    "    'ask_train.pt', 'ask_val.pt', 'ask_test.pt',\n",
    "]\n",
    "\n",
    "for dataset_scope in dataset_scopes:\n",
    "    dataset = []\n",
    "    for dataset_component in dataset_components:\n",
    "        dataset.append(\"datasets/\" + MONTH + \"/\" + dataset_scope + \"/\" + dataset_component)\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e71aa7a-5a5b-424b-af7c-d8a8299841ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datasets/jan/minimal/X_train.pt',\n",
       " 'datasets/jan/minimal/X_validation.pt',\n",
       " 'datasets/jan/minimal/X_test.pt',\n",
       " 'datasets/jan/minimal/y_train.pt',\n",
       " 'datasets/jan/minimal/y_validation.pt',\n",
       " 'datasets/jan/minimal/y_test.pt',\n",
       " 'datasets/jan/minimal/bid_train.pt',\n",
       " 'datasets/jan/minimal/bid_val.pt',\n",
       " 'datasets/jan/minimal/bid_test.pt',\n",
       " 'datasets/jan/minimal/ask_train.pt',\n",
       " 'datasets/jan/minimal/ask_val.pt',\n",
       " 'datasets/jan/minimal/ask_test.pt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf160cb0-9cf7-4108-b159-28813b07d299",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Feature engineering\n",
    "\n",
    "**I do not know if standardization is actually a good thing. [CHECK - Ask prof too]**  \n",
    "Standardization will be done using a small window of previous events. This is because the statistical properties change as time moves on, so we standardize w.r.t small prior windows. Standardization benefits ML training since values will stradle 0 and not be too far from it.  \n",
    "\n",
    "**Check if we should reindex time-series data with forward padding and explicitly 0-labels given to these regions.**\n",
    "\n",
    "**BE CAREFUL OF THE CURSE OF DIMENSIONALITY**\n",
    "To understand if the curse of dimensionality is affecting performance, rerun the exact same experiments with fewer features and see if profitability increases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa823b85-caf0-4604-80f7-1c2147a354c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = pd.read_pickle(f\"../data/AMD_{MONTH}_msg.pickle\")\n",
    "orderbook = pd.read_pickle(f\"../data/AMD_{MONTH}_odb.pickle\")\n",
    "\n",
    "market_open_idx = message.set_index('time').index.indexer_between_time(MARKET_OPEN, MARKET_CLOSE)\n",
    "\n",
    "message = message.iloc[market_open_idx, :].reset_index(drop=True)\n",
    "orderbook = orderbook.iloc[market_open_idx, :].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a2beb-e7b5-4718-94be-2282020c14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_data(message, orderbook, granularity=GRANULARITY, standardize_offset = 1) # Standardize by data of 1 day prior\n",
    "\n",
    "bids = data.loc[:, 'bid']\n",
    "asks = data.loc[:, 'ask']\n",
    "data = data.drop(['bid', 'ask'], axis=1)\n",
    "# data.to_pickle(\"datasets/jan/full/dataframe.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1fb670-8efb-4ebc-9e10-4dd62ad0184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For optimizing the threshold value to produce balanced class labels.\n",
    "\n",
    "Use with scipy.optimize.minimize_scalar\n",
    "\"\"\"\n",
    "def label_eval(threshold, time_horizon = 50):\n",
    "    labels = get_label(data.mid_price, time_horizon = time_horizon, threshold = threshold)\n",
    "    counts = labels.value_counts()\n",
    "    return -(counts / counts.sum()).min()\n",
    "\n",
    "threshold = minimize_scalar(label_eval, bounds=(0, 2)).x\n",
    "labels = get_label(data.mid_price, time_horizon=50, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b0f75-49fc-4d95-8c8a-d44c78ed8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913cd36b-cb1d-4365-9e4c-1853470f3d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, bids, asks = generate_time_series_samples(data, labels, bids, asks, SAMPLE_SIZE) # This fn performs dropna\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd010e8d-9554-4155-ac6a-72de45e9e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (y.index == asks.index).all()\n",
    "assert (y.index == bids.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb250f-df69-461d-8e39-86b263511c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flatten = X.reshape(X.shape[0], -1)\n",
    "X_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8695be0-f655-4aae-b1e8-4f911785ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_torch = torch.ones((X_flatten.shape[0], X_flatten.shape[1] + 1))\n",
    "X_torch[:, :-1] = torch.tensor(X_flatten)\n",
    "X_torch = X_torch.to(device)\n",
    "X_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c65a05e-7e1a-45f2-b85e-f53cda6e0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape, bids.shape, asks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd6251-3acf-4093-98f5-4dca317e9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6442491-f162-41a0-a129-2b7a3c8f6ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tensor = torch.FloatTensor(y + 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a691bd6-2116-442e-8cd1-0098f6d86a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tv, X_test, y_tv, y_test, bid_tv, bid_test, ask_tv, ask_test = train_test_split(X_torch, y_tensor, bids, asks, shuffle=False, test_size=0.2)\n",
    "X_train, X_val, y_train, y_val, bid_train, bid_val, ask_train, ask_val = train_test_split(X_tv, y_tv, bid_tv, ask_tv, shuffle=False, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0286b72-1fcb-428d-a37d-3fc264c32315",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datasets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e5e10-fea6-4faf-aecb-41847f66f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate([X_train, X_val, X_test, y_train, y_val, y_test, bid_train, bid_val, bid_test, ask_train, ask_val, ask_test]):\n",
    "    torch.save(data, dt[i])\n",
    "    print(type(data), data.shape, dt[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f940bcf6-ce29-4320-8d92-ed2956603ab2",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Note that actions are:\n",
    "- 0: Short   (Have -1 asset)\n",
    "- 1: Neutral (Have 0 asset)\n",
    "- 2: Long    (Have 1 asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09b0bcf8-47ba-469e-a674-747b5519c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\"\"\"Action-Value Function := Q(s, a)\"\"\"\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_dim),\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear_stack(x)\n",
    "\n",
    "\"\"\"Value Function := V(s)\"\"\"\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear_stack(x)\n",
    "\n",
    "\"\"\"Policy Function := P(a | s)\"\"\"\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_dim),\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x, return_logits=False):\n",
    "        logits = self.linear_stack(x)\n",
    "        if return_logits:\n",
    "            return logits\n",
    "        \n",
    "        probs = Categorical(logits=logits)\n",
    "        action = probs.sample()\n",
    "        return action\n",
    "    \n",
    "    def get_log_prob(self, state, action):\n",
    "        logits = self.linear_stack(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        return dist.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb880ebe-f412-4a5f-9dae-855ba07b62d0",
   "metadata": {},
   "source": [
    "## Reward function\n",
    "\n",
    "State representation:\n",
    "\n",
    "- At any current point we want to know our current portfolio\n",
    "- The action is how we want to the new portfolio that we want, we will limit our portfolio (and hence actions) to -1, 0, and 1 quantity of the asset  \n",
    "- We look at the next time step and get reward from the action we took\n",
    "\n",
    "This neutral penalty seems to work: $0.0001 * 10^{epoch}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d6a0cef-00e7-49f1-95bc-6afd27edbf3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_reward(prev_state, prev_bid, prev_ask, curr_state, curr_bid, curr_ask, use_midprice=False, prev_action_idx=-1, trading_fee=1e-4, neutral_penalty=1e-5):\n",
    "    valid_actions = [0, 1, 2]\n",
    "    assert (prev_state.dim() == 1) and (curr_state.dim() == 1), \"State provided is not 1 dimensional\"\n",
    "    assert (prev_state[prev_action_idx] in valid_actions) and (curr_state[prev_action_idx] in valid_actions), \"Action provided is not in [0, 1, 2] range\"\n",
    "    \n",
    "    prev_is_neutral = prev_state[prev_action_idx] == 1\n",
    "    curr_is_neutral = curr_state[prev_action_idx] == 1\n",
    "    \n",
    "    spread = 0 if use_midprice else (prev_ask - prev_bid)\n",
    "    \n",
    "    if curr_is_neutral:\n",
    "        if prev_is_neutral:\n",
    "            # Neutral -> Neutral\n",
    "            return -neutral_penalty\n",
    "\n",
    "        if not prev_is_neutral:\n",
    "            # Active -> Neutral\n",
    "            return -(trading_fee)\n",
    "    \n",
    "    # Here onwards current portfolio is active\n",
    "    curr_is_long = curr_state[prev_action_idx] == 2 # True -> Long, False -> Short\n",
    "    if use_midprice and curr_is_long:\n",
    "        price_change = ((curr_ask + curr_bid) / 2) - ((prev_ask + prev_bid) / 2)\n",
    "    elif use_midprice and not curr_is_long:\n",
    "        price_change = ((prev_ask + prev_bid) / 2) - ((curr_ask + curr_bid) / 2)\n",
    "    elif not use_midprice and curr_is_long:\n",
    "        price_change = curr_ask - prev_ask\n",
    "    else:\n",
    "        price_change = prev_bid - curr_bid\n",
    "        \n",
    "    portfolio_is_same = prev_state[prev_action_idx] == curr_state[prev_action_idx]\n",
    "    \n",
    "    if prev_is_neutral:\n",
    "        # Neutral -> Active\n",
    "        return -trading_fee + price_change\n",
    "    \n",
    "    if not prev_is_neutral:\n",
    "        if portfolio_is_same:\n",
    "            # Active -> Active\n",
    "            return price_change\n",
    "        else:\n",
    "            # Active -> -Active\n",
    "            return -(spread + trading_fee) - trading_fee + price_change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17da243-b7cb-4e0e-b9aa-36df24d98386",
   "metadata": {},
   "source": [
    "## 1. Advantage Actor Critic (A2C) RL\n",
    "\n",
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a82735c-3427-4ec5-8ab1-176eebb4247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_SPACE = [0, 1, 2]\n",
    "\n",
    "OUTPUT_DIMS = 3\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "N_PRETRAIN_EPOCH = 15\n",
    "\n",
    "N_EPOCHS = 110\n",
    "EPOCH_TRAIN_START = 5\n",
    "LEARNING_RATE = 3e-6\n",
    "REPLAY_MEMORY_SIZE = 1048576\n",
    "DISCOUNT_RATE = 0.9999       # Discounted rate of future returns\n",
    "FROZEN_UPDATE_INTERVAL = 8\n",
    "\n",
    "TRADING_FEE = 3e-5\n",
    "NEUTRAL_PENALTY = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f30a84-faf4-4a99-a1d8-7f430fe176e3",
   "metadata": {},
   "source": [
    "### Exploration-exploitation parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1de2552-f20e-4b0f-86cf-69d5bbb86311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_exploration_rate = 0.001\n",
    "max_exploration_rate = 1.\n",
    "exploration_decay_rate = 0.03\n",
    "ers = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*(epoch - EPOCH_TRAIN_START))\n",
    "    ers.append(exploration_rate)\n",
    "pd.Series(ers).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2bff5-8406-4e66-830d-8c8451d614a6",
   "metadata": {},
   "source": [
    "### Pretrain Actor Network then commence RL training\n",
    "\n",
    "Use the labels computed above to pretrain the actor network.  \n",
    "All portfolio values [0, 1, 2] are trained to output logits that best suit the given labels.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8d7ce-8cdb-4810-89bc-4b53ce556881",
   "metadata": {},
   "source": [
    "# TRY THE FOLLOWING WITHOUT PRETRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d88a240-a3db-4d6b-9686-bfceea21a770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 Batch:  25 of 29 Loss:   1.828516\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"A2C\")\n",
    "for dataset in datasets:\n",
    "    for USE_MIDPRICE in [False, True]:\n",
    "        # Fetch data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, bid_train, bid_val, bid_test, ask_train, ask_val, ask_test = [torch.load(path) for path in dataset]\n",
    "        y_train, y_val, y_test = y_train.long(), y_val.long(), y_test.long()\n",
    "        INPUT_DIMS = X_train.shape[-1]\n",
    "\n",
    "        policy = ActorNetwork(input_dim=INPUT_DIMS, output_dim=OUTPUT_DIMS).to(device)\n",
    "        critic = CriticNetwork(input_dim=INPUT_DIMS).to(device)\n",
    "        critic_frozen = CriticNetwork(input_dim=INPUT_DIMS).to(device)\n",
    "        critic_frozen.load_state_dict(critic.state_dict())\n",
    "        \n",
    "                \n",
    "        # Begin logging\n",
    "        with mlflow.start_run() as run:\n",
    "            mlflow.log_params({\n",
    "                'RL_algorithm': \"A2C\",\n",
    "                'batch_size': BATCH_SIZE,\n",
    "                'n_pretrain_epoch': N_PRETRAIN_EPOCH,\n",
    "                'n_epochs': N_EPOCHS,\n",
    "                'train_start_epoch': EPOCH_TRAIN_START,\n",
    "                'learning_rate': LEARNING_RATE,\n",
    "                'replay_memory_size': REPLAY_MEMORY_SIZE,\n",
    "                'discount_rate': DISCOUNT_RATE,\n",
    "                'frozen_update_interval': FROZEN_UPDATE_INTERVAL,\n",
    "                'use_midprice': USE_MIDPRICE,\n",
    "                'trading_fee': TRADING_FEE,\n",
    "                'neutral_penalty': NEUTRAL_PENALTY,\n",
    "                'market_open': MARKET_OPEN,\n",
    "                'market_close': MARKET_CLOSE,\n",
    "                'granularity': GRANULARITY,\n",
    "                'month': MONTH,\n",
    "                'datascope': dataset[0].split(\"/\")[2],\n",
    "                'device': device,\n",
    "                'input_dims': INPUT_DIMS,\n",
    "                'critic_loss_fn': 'MSE',\n",
    "                'network_optimizer_fns': 'Adam',\n",
    "                'pretrain_loss_fn': 'CrossEntropyLoss',\n",
    "                'policy_network': str(policy.linear_stack).replace(\"in_features=\", \"\").replace(\"out_features=\", \"\").replace(\" \", \"\").replace(\"\\n\", \"\").replace(\",bias=True\", \"\"),\n",
    "                'critic_network': str(critic.linear_stack).replace(\"in_features=\", \"\").replace(\"out_features=\", \"\").replace(\" \", \"\").replace(\"\\n\", \"\").replace(\",bias=True\", \"\"),\n",
    "                'min_exploration_rate': min_exploration_rate,\n",
    "                'max_exploration_rate': max_exploration_rate,\n",
    "                'exploration_decay_rate': exploration_decay_rate,\n",
    "            })\n",
    "\n",
    "            policy.train()\n",
    "            critic.train()\n",
    "            critic_frozen.eval()\n",
    "\n",
    "            policy_optimizer = optim.Adam(params=policy.parameters(), lr=LEARNING_RATE)\n",
    "            critic_optimizer = optim.Adam(params=critic.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "            mse_fn = nn.MSELoss()\n",
    "\n",
    "            dataloader = DataLoader([(X_train[i], y_train[i]) for i in range(X_train.shape[0])], shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "            pretrain_loss_fn = nn.CrossEntropyLoss()\n",
    "            pretrain_optimizer = optim.Adam(params=policy.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "            for i in range(N_PRETRAIN_EPOCH):\n",
    "                for batch, (X, y) in enumerate(dataloader):\n",
    "                    X_dummy = X.clone()\n",
    "                    X_dummy[..., -1] = i % 3\n",
    "\n",
    "                    logits = policy(X_dummy, return_logits=True)\n",
    "                    pretrain_loss = pretrain_loss_fn(logits, y.long())\n",
    "\n",
    "                    # Backpropagation\n",
    "                    pretrain_optimizer.zero_grad()\n",
    "                    pretrain_loss.backward()\n",
    "                    pretrain_optimizer.step()\n",
    "\n",
    "                    if batch % 5 == 0:\n",
    "                        print(f\"Epoch: {i:2} Batch: {batch:3} of {len(dataloader)} Loss: {pretrain_loss.item():10.6f}\", end=\"\\r\")\n",
    "                mlflow.log_metric(\"pretrain_loss\", pretrain_loss.item(), step=i)\n",
    "\n",
    "            print(\"\")\n",
    "\n",
    "            critic_loss_history = []\n",
    "            policy_loss_history = []\n",
    "            reward_history = []\n",
    "            replay_memory = ReplayMemory(REPLAY_MEMORY_SIZE) # 2**20 values\n",
    "            \n",
    "            best_episode_reward = float(\"-inf\")\n",
    "            best_episode_validation_reward = float(\"-inf\")\n",
    "\n",
    "            for epoch in range(N_EPOCHS):\n",
    "                policy.train()\n",
    "                episode_action_history = []\n",
    "                done = False\n",
    "                episode_reward = 0\n",
    "                state = X_train[0].clone()\n",
    "                bid, ask = bid_train[0], ask_train[0]\n",
    "\n",
    "                for i in range(1, X_train.shape[0]):\n",
    "                    # Explore vs exploit\n",
    "                    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*(epoch - EPOCH_TRAIN_START))\n",
    "                    if random.uniform(0, 1) > exploration_rate:\n",
    "                        action = policy(state) # Exploit\n",
    "                    else:\n",
    "                        action = torch.tensor(random.sample(ACTION_SPACE, 1)).to(device) # Explore\n",
    "\n",
    "                    # Environment step\n",
    "                    next_state = X_train[i].clone()\n",
    "                    next_state[..., -1] = action\n",
    "                    next_bid = bid_train[i]\n",
    "                    next_ask = ask_train[i]\n",
    "\n",
    "                    reward = get_reward(state, bid, ask, next_state, next_bid, next_ask, use_midprice=USE_MIDPRICE, trading_fee=TRADING_FEE, neutral_penalty=NEUTRAL_PENALTY)\n",
    "                    done = i == (X_train.shape[0] - 1)\n",
    "\n",
    "                    # Saves\n",
    "                    replay_memory.push(Experience(state, action, reward, next_state, done))\n",
    "                    episode_action_history.append(action.item())\n",
    "                    episode_reward += reward\n",
    "                    state = next_state\n",
    "                    bid = next_bid\n",
    "                    ask = next_ask\n",
    "\n",
    "                    if replay_memory.can_provide_sample(BATCH_SIZE) and epoch > EPOCH_TRAIN_START:\n",
    "                        states, actions, rewards, next_states, dones = replay_memory.sample(BATCH_SIZE)\n",
    "                        states, actions, rewards, next_states, dones = states.to(device), actions.to(device), rewards.to(device), next_states.to(device), dones.to(device)\n",
    "                        actions = actions.long()\n",
    "                        dones = dones.float()\n",
    "\n",
    "                        # Error calculation\n",
    "                        log_probs = policy.get_log_prob(states, actions)\n",
    "                        q_values = critic(states).squeeze(-1)\n",
    "                        q_target = (rewards + (1 - dones) * DISCOUNT_RATE * critic_frozen(next_states).squeeze(-1)).detach()\n",
    "                        advantages = q_target - q_values\n",
    "\n",
    "                        assert (log_probs <= 0).all(), \"There cannot be positive log probabilities\"\n",
    "\n",
    "                        policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "                        value_loss = mse_fn(q_target, q_values)\n",
    "\n",
    "                        # Optimization\n",
    "                        critic_optimizer.zero_grad()\n",
    "                        value_loss.backward()\n",
    "                        critic_optimizer.step()\n",
    "\n",
    "                        policy_optimizer.zero_grad()\n",
    "                        policy_loss.backward()\n",
    "                        policy_optimizer.step()\n",
    "\n",
    "                reward_history.append(episode_reward) ######## LOG THE REWARD\n",
    "                mlflow.log_metric(\"episode_reward\", episode_reward, step=epoch)\n",
    "\n",
    "                if replay_memory.can_provide_sample(BATCH_SIZE) and i % FROZEN_UPDATE_INTERVAL == 0 and epoch > EPOCH_TRAIN_START:\n",
    "                    critic_frozen.load_state_dict(critic.state_dict())\n",
    "                    critic_frozen.eval()\n",
    "\n",
    "                if replay_memory.can_provide_sample(BATCH_SIZE) and epoch > EPOCH_TRAIN_START:\n",
    "                        print(f\"Epoch: {epoch:>5,d}, Reward: {episode_reward:>10,.6f}, Critic loss: {value_loss.item():>10,.6f}, Actor loss: {policy_loss.item():>10,.6f}\", end=\"\\r\")\n",
    "                        mlflow.log_metric(\"critic_loss\", value_loss.item(), step=epoch)\n",
    "                        mlflow.log_metric(\"actor_loss\", policy_loss.item(), step=epoch)\n",
    "                        \n",
    "                        # Save episode action information - Need to see if the agent is changing its portfolio regularly\n",
    "                        for k, v in dict(pd.Series(episode_action_history).value_counts()).items():\n",
    "                            mlflow.log_metric(f\"action_{k}\", v, step=epoch)\n",
    "                            \n",
    "                if (episode_reward > best_episode_reward):\n",
    "                    best_episode_reward = episode_reward\n",
    "                    torch.save(policy.state_dict(), \"best_train_policy_state_dict.pt\")\n",
    "                    torch.save(critic.state_dict(), \"best_train_critic_state_dict.pt\")\n",
    "                    mlflow.log_artifact(\"best_train_policy_state_dict.pt\")\n",
    "                    mlflow.log_artifact(\"best_train_critic_state_dict.pt\")\n",
    "################\n",
    "                with torch.no_grad():\n",
    "                    # Validation checks\n",
    "                    policy.eval()\n",
    "                    val_state = X_val[0]\n",
    "                    val_bid = bid_val[0]\n",
    "                    val_ask = ask_val[0]\n",
    "                    validation_reward = 0\n",
    "                    validation_actions = []\n",
    "\n",
    "                    for k in range(1, X_val.shape[0]):\n",
    "                        val_action = policy(val_state)\n",
    "\n",
    "                        val_next_state = X_val[k]\n",
    "                        val_next_state[..., -1] = val_action\n",
    "\n",
    "                        val_next_bid = bid_val[k]\n",
    "                        val_next_ask = ask_val[k]\n",
    "\n",
    "                        reward = get_reward(val_state, val_bid, val_ask, val_next_state, val_next_bid, val_next_ask, neutral_penalty=0)\n",
    "                        validation_reward += reward\n",
    "                        validation_actions.append(val_action.item())\n",
    "                        val_state = val_next_state\n",
    "                        val_bid = val_next_bid\n",
    "                        val_ask = val_next_ask\n",
    "\n",
    "                    mlflow.log_metric(\"validation_episode_reward\", validation_reward, step=epoch)\n",
    "                    for key, value in dict(pd.Series(validation_actions).value_counts()).items():\n",
    "                        mlflow.log_metric(f\"validation_action_{key}\", value, step=epoch)\n",
    "\n",
    "                    if validation_reward > best_episode_validation_reward:\n",
    "                        best_episode_validation_reward = validation_reward\n",
    "                        torch.save(policy.state_dict(), \"best_val_policy_state_dict.pt\")\n",
    "                        torch.save(critic.state_dict(), \"best_val_critic_state_dict.pt\")\n",
    "                        mlflow.log_artifact(\"best_val_policy_state_dict.pt\")\n",
    "                        mlflow.log_artifact(\"best_val_critic_state_dict.pt\")\n",
    "                    policy.train()\n",
    "\n",
    "            torch.save(policy.state_dict(), \"last_train_policy_state_dict.pt\")\n",
    "            torch.save(critic.state_dict(), \"last_train_critic_state_dict.pt\")\n",
    "            mlflow.log_artifact(\"last_train_policy_state_dict.pt\")\n",
    "            mlflow.log_artifact(\"last_train_critic_state_dict.pt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d73457-bb4d-4158-b9b8-c4133e847461",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50e1ec8-6d0e-4a86-8287-7e8b4533f121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    14, Reward: -29.872640, Value loss:   0.241062\r"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"Double DQN\")\n",
    "for dataset in datasets:\n",
    "    for USE_MIDPRICE in [False, True]:\n",
    "        # Fetch data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test, bid_train, bid_val, bid_test, ask_train, ask_val, ask_test = [torch.load(path) for path in dataset]\n",
    "        y_train, y_val, y_test = y_train.long(), y_val.long(), y_test.long()\n",
    "        INPUT_DIMS = X_train.shape[-1]\n",
    "        \n",
    "        value_network = ValueNetwork(input_dim = INPUT_DIMS, output_dim = OUTPUT_DIMS).to(device)\n",
    "        fixed_network = ValueNetwork(input_dim = INPUT_DIMS, output_dim = OUTPUT_DIMS).to(device)\n",
    "        fixed_network.load_state_dict(value_network.state_dict())\n",
    "        value_network.train()\n",
    "        fixed_network.eval()\n",
    "        \n",
    "        value_optimizer = optim.Adam(params=value_network.parameters(), lr=LEARNING_RATE)\n",
    "        mse_fn = nn.MSELoss()\n",
    "        \n",
    "        # Begin logging\n",
    "        with mlflow.start_run() as run:\n",
    "            mlflow.log_params({\n",
    "                'batch_size': BATCH_SIZE,\n",
    "                'n_epochs': N_EPOCHS,\n",
    "                'train_start_epoch': EPOCH_TRAIN_START,\n",
    "                'learning_rate': LEARNING_RATE,\n",
    "                'replay_memory_size': REPLAY_MEMORY_SIZE,\n",
    "                'discount_rate': DISCOUNT_RATE,\n",
    "                'frozen_update_interval': FROZEN_UPDATE_INTERVAL,\n",
    "                'use_midprice': USE_MIDPRICE,\n",
    "                'trading_fee': TRADING_FEE,\n",
    "                'neutral_penalty': NEUTRAL_PENALTY,\n",
    "                'market_open': MARKET_OPEN,\n",
    "                'market_close': MARKET_CLOSE,\n",
    "                'granularity': GRANULARITY,\n",
    "                'month': MONTH,\n",
    "                'datascope': dataset[0].split(\"/\")[2],\n",
    "                'device': device,\n",
    "                'input_dims': INPUT_DIMS,\n",
    "                'critic_loss_fn': 'MSE',\n",
    "                'network_optimizer_fns': 'Adam',\n",
    "                'pretrain_loss_fn': 'CrossEntropyLoss',\n",
    "                'policy_network': str(value_network.linear_stack).replace(\"in_features=\", \"\").replace(\"out_features=\", \"\").replace(\" \", \"\").replace(\"\\n\", \"\").replace(\",bias=True\", \"\"),\n",
    "                'min_exploration_rate': min_exploration_rate,\n",
    "                'max_exploration_rate': max_exploration_rate,\n",
    "                'exploration_decay_rate': exploration_decay_rate,\n",
    "            })\n",
    "\n",
    "            critic_loss_history = []\n",
    "            policy_loss_history = []\n",
    "            reward_history = []\n",
    "            replay_memory = ReplayMemory(REPLAY_MEMORY_SIZE) # 2**20 values\n",
    "            \n",
    "            best_episode_reward = float(\"-inf\")\n",
    "            best_episode_validation_reward = float(\"-inf\")\n",
    "\n",
    "            for epoch in range(N_EPOCHS):\n",
    "                value_network.train()\n",
    "                episode_action_history = []\n",
    "                done = False\n",
    "                episode_reward = 0\n",
    "                state = X_train[0].clone()\n",
    "                bid, ask = bid_train[0], ask_train[0]\n",
    "\n",
    "                for i in range(1, X_train.shape[0]):\n",
    "                    # Explore vs exploit\n",
    "                    exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*(epoch - EPOCH_TRAIN_START))\n",
    "                    if random.uniform(0, 1) > exploration_rate:\n",
    "                        value_network(state).argmax()\n",
    "                    else:\n",
    "                        action = torch.tensor(random.sample(ACTION_SPACE, 1)).to(device) # Explore\n",
    "\n",
    "                    # Environment step\n",
    "                    next_state = X_train[i].clone()\n",
    "                    next_state[..., -1] = action\n",
    "                    next_bid = bid_train[i]\n",
    "                    next_ask = ask_train[i]\n",
    "\n",
    "                    reward = get_reward(state, bid, ask, next_state, next_bid, next_ask, use_midprice=USE_MIDPRICE, trading_fee=TRADING_FEE, neutral_penalty=NEUTRAL_PENALTY)\n",
    "                    done = i == (X_train.shape[0] - 1)\n",
    "\n",
    "                    # Saves\n",
    "                    replay_memory.push(Experience(state, action, reward, next_state, done))\n",
    "                    episode_action_history.append(action.item())\n",
    "                    episode_reward += reward\n",
    "                    state = next_state\n",
    "                    bid = next_bid\n",
    "                    ask = next_ask\n",
    "\n",
    "                    if replay_memory.can_provide_sample(BATCH_SIZE) and epoch > EPOCH_TRAIN_START:\n",
    "                        states, actions, rewards, next_states, dones = replay_memory.sample(BATCH_SIZE)\n",
    "                        states, actions, rewards, next_states, dones = states.to(device), actions.to(device), rewards.to(device), next_states.to(device), dones.to(device)\n",
    "                        actions = actions.long()\n",
    "                        dones = dones.float()\n",
    "\n",
    "                        # Error calculation\n",
    "                        action_targets = value_network(next_states).max(dim=-1)[1].unsqueeze(-1)\n",
    "                        target_values = rewards + (1 - dones) * DISCOUNT_RATE * fixed_network(next_states).gather(-1, action_targets).squeeze(-1) # Double DQN\n",
    "                        # target_values = rewards + (1 - dones) * DISCOUNT_RATE * q_fn_frozen(next_states).max(dim=-1)[0] # DQN\n",
    "\n",
    "                        current_values = value_network(states).gather(-1, actions.unsqueeze(-1)).squeeze(1)\n",
    "\n",
    "                        # Loss\n",
    "                        loss = mse_fn(current_values, target_values.detach())\n",
    "                        value_optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        value_optimizer.step()\n",
    "\n",
    "                reward_history.append(episode_reward) ######## LOG THE REWARD\n",
    "                mlflow.log_metric(\"episode_reward\", episode_reward, step=epoch)\n",
    "\n",
    "                if replay_memory.can_provide_sample(BATCH_SIZE) and i % FROZEN_UPDATE_INTERVAL == 0 and epoch > EPOCH_TRAIN_START:\n",
    "                    fixed_network.load_state_dict(value_network.state_dict())\n",
    "                    fixed_network.eval()\n",
    "\n",
    "                if replay_memory.can_provide_sample(BATCH_SIZE) and epoch > EPOCH_TRAIN_START:\n",
    "                        print(f\"Epoch: {epoch:>5,d}, Reward: {episode_reward:>10,.6f}, Value loss: {loss.item():>10,.6f}\", end=\"\\r\")\n",
    "                        mlflow.log_metric(\"train_value_network_loss\", loss.item(), step=epoch)\n",
    "                        \n",
    "                        # Save episode action information - Need to see if the agent is changing its portfolio regularly\n",
    "                        for k, v in dict(pd.Series(episode_action_history).value_counts()).items():\n",
    "                            mlflow.log_metric(f\"action_{k}\", v, step=epoch)\n",
    "                            \n",
    "                if (episode_reward > best_episode_reward):\n",
    "                    best_episode_reward = episode_reward\n",
    "                    torch.save(value_network.state_dict(), \"best_train_value_state_dict.pt\")\n",
    "                    mlflow.log_artifact(\"best_train_value_state_dict.pt\")\n",
    "################\n",
    "                with torch.no_grad():\n",
    "                    # Validation checks\n",
    "                    value_network.eval()\n",
    "                    val_state = X_val[0]\n",
    "                    val_bid = bid_val[0]\n",
    "                    val_ask = ask_val[0]\n",
    "                    validation_reward = 0\n",
    "                    validation_actions = []\n",
    "\n",
    "                    for k in range(1, X_val.shape[0]):\n",
    "                        val_action = value_network(val_state).argmax()\n",
    "\n",
    "                        val_next_state = X_val[k]\n",
    "                        val_next_state[..., -1] = val_action\n",
    "\n",
    "                        val_next_bid = bid_val[k]\n",
    "                        val_next_ask = ask_val[k]\n",
    "\n",
    "                        reward = get_reward(val_state, val_bid, val_ask, val_next_state, val_next_bid, val_next_ask, neutral_penalty=0)\n",
    "                        validation_reward += reward\n",
    "                        validation_actions.append(val_action.item())\n",
    "                        val_state = val_next_state\n",
    "                        val_bid = val_next_bid\n",
    "                        val_ask = val_next_ask\n",
    "\n",
    "                    mlflow.log_metric(\"validation_episode_reward\", validation_reward, step=epoch)\n",
    "                    for key, value in dict(pd.Series(validation_actions).value_counts()).items():\n",
    "                        mlflow.log_metric(f\"validation_action_{key}\", value, step=epoch)\n",
    "\n",
    "                    if validation_reward > best_episode_validation_reward:\n",
    "                        best_episode_validation_reward = validation_reward\n",
    "                        torch.save(value_network.state_dict(), \"best_val_value_state_dict.pt\")\n",
    "                        mlflow.log_artifact(\"best_val_value_state_dict.pt\")\n",
    "                    value_network.train()\n",
    "\n",
    "            torch.save(value_network.state_dict(), \"last_train_value_state_dict.pt\")\n",
    "            mlflow.log_artifact(\"last_train_value_state_dict.pt\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e44b0-d3ed-4521-9fa3-61cb55d775a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b60964-a28f-4d27-b02c-237237b2f148",
   "metadata": {},
   "source": [
    "Final report\n",
    "- Describe RL\n",
    "- Describe LOB\n",
    "- What metric are we trying to optimise\n",
    "- Inputs\n",
    "- Literature review\n",
    "- Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
